{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Model Comparison for Robot Policy Performance\n",
                "\n",
                "This notebook compares the performance of different models tested using the `eval.py` script.\n",
                "\n",
                "We load all model results from the `results/` dir and concatenate them into a dataframe. Repeats of the same title (such as f\"{model_name}_{history_length}_{history_choice})\" are averaged into one result for the downstream metrics and renamed to `\"...-avg-N\".`"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 267,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import pickle\n",
                "import os\n",
                "import pandas as pd\n",
                "import seaborn as sns\n",
                "from typing import Dict, List, Any\n",
                "\n",
                "# Set plot style\n",
                "plt.style.use('ggplot')\n",
                "sns.set_context(\"talk\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def calc_expected_frames(history_length: int, history_choice: str) -> int:\n",
                "    if history_choice == \"all\":\n",
                "        return history_length\n",
                "    elif history_choice == \"last\":\n",
                "        return 1\n",
                "    elif history_choice == \"first\":\n",
                "        return 1\n",
                "    elif history_choice == \"alternate\":\n",
                "        return history_length // 2\n",
                "    elif history_choice == \"third\":\n",
                "        return history_length // 3\n",
                "    elif history_choice.lower() == \"none\":\n",
                "        return 0\n",
                "    elif history_choice.lower() == \"one\":\n",
                "        return 1\n",
                "    else:\n",
                "        raise ValueError(f\"Invalid history_choice: {history_choice}\")\n",
                "\n",
                "RESULTS_DIR = \"results\"\n",
                "RESULTS = dict()\n",
                "model_dirs = [mdir for mdir in os.listdir(RESULTS_DIR) if os.path.isdir(os.path.join(RESULTS_DIR, mdir))]\n",
                "for model_name in model_dirs:\n",
                "    print(f\"loading results for {model_name}\")\n",
                "    if os.path.isdir(os.path.join(RESULTS_DIR, model_name)):\n",
                "        if model_name.replace(\"history_sweep_\", \"\") not in RESULTS:\n",
                "            RESULTS[model_name.replace(\"history_sweep_\", \"\")] = list()\n",
                "        for filename in os.listdir(os.path.join(RESULTS_DIR, model_name)):\n",
                "            if 'metrics' in filename and filename.endswith('.pkl'):\n",
                "                print(f\"found metrics file {os.path.join(RESULTS_DIR, model_name, filename)}\")\n",
                "                metrics_dict = pickle.load(open(os.path.join(RESULTS_DIR, model_name, filename), \"rb\"))\n",
                "                actions_file = filename.replace('metrics', 'actions')\n",
                "                actions_dict = pickle.load(open(os.path.join(RESULTS_DIR, model_name, actions_file), \"rb\"))\n",
                "                info_dict = {k: metrics_dict[k] for k in ['timestamp', 'model_name', 'history_length', 'history_choice']}\n",
                "                info_dict['model_name'] = model_name.replace(\"history_sweep_\", \"\")\n",
                "                RESULTS[model_name.replace(\"history_sweep_\", \"\")].append({\"info\": info_dict, \"actions\": actions_dict, \"metrics\": metrics_dict})\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def info_dict_to_title(info_dict):\n",
                "    return f\"{info_dict['model_name']}\" + (f\"-histlen-{info_dict['history_length']}-histchoice-{info_dict['history_choice']}\" if info_dict['history_length'] is not None else f\"\")\n",
                "\n",
                "# Check which models were loaded successfully\n",
                "print(f\"Successfully loaded data for {len(RESULTS)} models and {sum([len(v) for v in RESULTS.values()])} results\")\n",
                "for model_name, results_list in RESULTS.items():\n",
                "    print(f\"model name: {model_name}\")\n",
                "    for result in results_list:\n",
                "        print(f\"- {info_dict_to_title(result['info'])}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 270,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Function to compute metrics for models without pre-computed metrics\n",
                "def compute_metrics(pred_actions, gt_actions):\n",
                "    results = {}\n",
                "    \n",
                "    # MSE - Mean Squared Error\n",
                "    results[\"mse\"] = np.mean(np.square(pred_actions - gt_actions))\n",
                "    \n",
                "    # MAE - Mean Absolute Error\n",
                "    results[\"mae\"] = np.mean(np.abs(pred_actions - gt_actions))\n",
                "    \n",
                "    # Normalized MSE - divide by variance of ground truth\n",
                "    gt_var = np.var(gt_actions)\n",
                "    if gt_var > 0:\n",
                "        results[\"nmse\"] = results[\"mse\"] / gt_var\n",
                "    else:\n",
                "        results[\"nmse\"] = float('inf')\n",
                "    \n",
                "    # Action magnitude comparison\n",
                "    results[\"pred_mag\"] = np.mean(np.linalg.norm(pred_actions, axis=1))\n",
                "    results[\"gt_mag\"] = np.mean(np.linalg.norm(gt_actions, axis=1))\n",
                "    results[\"mag_ratio\"] = results[\"pred_mag\"] / results[\"gt_mag\"] if results[\"gt_mag\"] > 0 else float('inf')\n",
                "    \n",
                "    return results"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 271,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Extract and organize metrics across models for comparison\n",
                "def extract_comparison_metrics(model_data: dict[str, list[dict]], force_compute: bool = True, min_results: int = 4):\n",
                "    comparison = {}\n",
                "    \n",
                "    for model_name, model_results in model_data.items():\n",
                "        for data in model_results:\n",
                "            if data['info']['history_length'] is None:\n",
                "                data['info']['history_length'] = 0 \n",
                "            hist_len = data['info']['history_length']\n",
                "\n",
                "            if hist_len in (None, 0) or data['info']['history_choice'] is None:\n",
                "                data['info']['history_choice'] = 'None'\n",
                "            elif hist_len == 1:\n",
                "                data['info']['history_choice'] = 'all'\n",
                "            hist_choice = data['info']['history_choice']\n",
                "\n",
                "            model_title = info_dict_to_title(data['info'])\n",
                "            if model_title in comparison:\n",
                "                copies = [x for x in comparison.keys() if model_title == x or (model_title in x and len(x)- 2 == len(model_title))]\n",
                "                model_title = f\"{model_title}-{len(copies)}\"\n",
                "            # If we have pre-computed metrics, use those\n",
                "            if 'metrics' in data and 'avg_metrics' in data['metrics'] and not force_compute:\n",
                "                comparison[model_title] = data['metrics']['avg_metrics']\n",
                "            else:\n",
                "                # Otherwise, compute metrics from actions\n",
                "                all_results = []\n",
                "                \n",
                "                for traj in data['actions']:\n",
                "                    pred_actions = traj['pred_actions']\n",
                "                    gt_actions = traj['gt_actions']\n",
                "                    \n",
                "                    results = compute_metrics(pred_actions, gt_actions)\n",
                "                    \n",
                "                    # Add timing information if available\n",
                "                    if 'inference_times' in traj:\n",
                "                        results['mean_inference_time'] = np.mean(traj['inference_times'])\n",
                "                    all_results.append(results)\n",
                "                \n",
                "                # Compute average metrics across all trajectories\n",
                "                if len(all_results) < min_results:\n",
                "                    print(f\"Not enough results for {model_title}, skipping {len(all_results)} results\")\n",
                "                    continue\n",
                "                metrics = {k: np.mean([r[k] for r in all_results]) for k in all_results[0]}\n",
                "                # add info to the df\n",
                "                metrics['model_name'] = model_name\n",
                "                metrics['model_title'] = model_title\n",
                "                metrics['history_length'] = hist_len\n",
                "                metrics['history_choice'] = hist_choice\n",
                "                metrics['expected_frames'] = calc_expected_frames(hist_len, hist_choice) \n",
                "                comparison[model_title] = metrics\n",
                "\n",
                "    return comparison\n",
                "\n",
                "def consolidate_reruns(comparison: dict[str, dict[str, Any]]) -> dict[str, dict[str, Any]]:\n",
                "    averaged_comparison = dict()\n",
                "    multiples = [t for t in comparison.keys() if t.endswith('-1')]\n",
                "    for t in multiples:\n",
                "        base_title = t[:-2]\n",
                "        matched_titles = [x for x in comparison.keys() if x.startswith(base_title) and len(x) <= len(t)]\n",
                "        avg_metrics = dict()\n",
                "        for k in comparison[matched_titles[0]].keys():\n",
                "            if type(comparison[matched_titles[0]][k]) in (str, type(None)):\n",
                "                # string metrics like choice, name, title etc are the same\n",
                "                avg_metrics[k] = comparison[matched_titles[0]][k]\n",
                "            else:\n",
                "                avg_metrics[k] = np.mean([comparison[t][k] for t in matched_titles])\n",
                "\n",
                "        averaged_comparison[base_title + f\"-avg-{len(matched_titles)}\"] = avg_metrics\n",
                "        print(f\"averaged {len(matched_titles)} models for {base_title}\")\n",
                "        for m in matched_titles:\n",
                "            del comparison[m]\n",
                "    comparison.update(averaged_comparison)\n",
                "    return comparison"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Extract performance metrics for comparison\n",
                "all_comparison_metrics = extract_comparison_metrics(RESULTS)\n",
                "comparison_metrics = consolidate_reruns(all_comparison_metrics)\n",
                "\n",
                "metrics_of_interest = list(comparison_metrics.values())[0].keys()\n",
                "print(metrics_of_interest)\n",
                "\n",
                "raw_comparison_df = pd.DataFrame(index=metrics_of_interest)\n",
                "for model_name, metrics in all_comparison_metrics.items():\n",
                "    model_values = [metrics.get(metric, np.nan) for metric in metrics_of_interest]\n",
                "    raw_comparison_df[model_name] = model_values\n",
                "\n",
                "\n",
                "comparison_df = pd.DataFrame(index=metrics_of_interest)\n",
                "\n",
                "for model_name, metrics in comparison_metrics.items():\n",
                "    model_values = [metrics.get(metric, np.nan) for metric in metrics_of_interest]\n",
                "    comparison_df[model_name] = model_values\n",
                "\n",
                "# i messed up naming o4-mini\n",
                "comparison_df.loc['model_name'] = comparison_df.loc['model_name'].str.replace('4-mini', 'o4-mini')\n",
                "\n",
                "# Display the comparison table\n",
                "comparison_df"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "comparison_df.T.model_name.value_counts()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "VLA_MODEL_NAMES = [\"openvla\", \"ecot\"]\n",
                "existing_vlas = [m for m in comparison_df.T.model_name.unique() if any(vla in m for vla in VLA_MODEL_NAMES)]\n",
                "print(f\"found {len(existing_vlas)} VLA models: {existing_vlas}\")\n",
                "vla_df = comparison_df.T[comparison_df.T.model_name.isin(existing_vlas)].T\n",
                "\n",
                "BASELINE_METRICS = dict() # find the VLA metrics to use as baseline comparisons\n",
                "desired_metrics = [\"mse\"]\n",
                "for _, row in vla_df.T.iterrows():\n",
                "    BASELINE_METRICS[row['model_name']] = dict()\n",
                "    for metric in desired_metrics:\n",
                "        BASELINE_METRICS[row['model_name']][metric] = row[metric]\n",
                "\n",
                "print(f\"Collected baseline VLA metrics: {BASELINE_METRICS}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "model_name_counts = raw_comparison_df.T.model_name.value_counts()\n",
                "print(model_name_counts)\n",
                "print(f\"Total unique models: {len(model_name_counts)}\\n\")\n",
                "\n",
                "model_title_counts =raw_comparison_df.T.model_title.value_counts()\n",
                "print(f\"Total unique configurations: {len(model_title_counts)}\\n\")\n",
                "\n",
                "histlen_counts = raw_comparison_df.T.history_length.value_counts()\n",
                "print(histlen_counts)\n",
                "\n",
                "histchoice_counts = raw_comparison_df.T.history_choice.value_counts()\n",
                "print(histchoice_counts)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(comparison_df.T.model_name.value_counts())\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Performance Metric Comparisons"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 277,
            "metadata": {},
            "outputs": [],
            "source": [
                "from typing import Callable, Optional\n",
                "\n",
                "from typing import Callable, Optional, Tuple\n",
                "\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "from typing import Callable, Optional\n",
                "\n",
                "\n",
                "\n",
                "def plot_metric_comparison(comparison_df: pd.DataFrame, metric_name: str, title: Optional[str]=None, lower_is_better=True, groupby: Optional[str] = None, filterby: Optional[tuple[str, Callable]] = None, other_vals: Optional[dict[str, float]] = None):\n",
                "    plt.figure(figsize=(10, 6))\n",
                "    \n",
                "    # Get the data for this metric\n",
                "    if filterby is not None:\n",
                "        comparison_df = comparison_df.T[comparison_df.T.apply(filterby, axis=1)].T\n",
                "\n",
                "    if groupby is not None:\n",
                "        comparison_df = comparison_df.T.groupby([groupby])[[metric_name]].agg('mean').T\n",
                "\n",
                "    metric_values = comparison_df.loc[metric_name]\n",
                "    \n",
                "    # Sort models by metric value\n",
                "    if lower_is_better:\n",
                "        sorted_models = metric_values.sort_values().index\n",
                "    else:\n",
                "        sorted_models = metric_values.sort_values(ascending=False).index\n",
                "    \n",
                "    # Create bar plot\n",
                "    bars = plt.bar(sorted_models, metric_values[sorted_models])\n",
                "    \n",
                "    # Add value labels on top of bars with smaller font size\n",
                "    for bar in bars:\n",
                "        height = bar.get_height()\n",
                "        plt.text(bar.get_x() + bar.get_width()/2., height + 0.001,\n",
                "                f'{height:.4f}', ha='center', va='bottom', rotation=0, fontsize=8)\n",
                "    \n",
                "    # Add title and labels\n",
                "    plt.title(title or f'{metric_name.upper()} Comparison Across Models')\n",
                "    plt.ylabel(metric_name)\n",
                "    if groupby is not None:\n",
                "        plt.xlabel(groupby.replace('_', ' ').title())\n",
                "    \n",
                "    # Set rotated tick labels with proper alignment\n",
                "    plt.xticks(rotation=45, ha='right')\n",
                "\n",
                "    if other_vals is not None:\n",
                "        for k,v in other_vals.items():\n",
                "            plt.axhline(y=v, color='gray', linestyle='--', alpha=0.7, label=k)\n",
                "    plt.legend()\n",
                "\n",
                "    plt.tight_layout()\n",
                "    \n",
                "    return plt"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 278,
            "metadata": {},
            "outputs": [],
            "source": [
                "GROUPBY_OPTIONS = ['model_name', 'history_length', 'history_choice', 'expected_frames']"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Plot MSE comparison (lower is better)\n",
                "# filterby = lambda x: \"claude\" in x['model_name'] and 'first' in x['history_choice']\n",
                "filterby = None\n",
                "groupby = \"model_name\"\n",
                "\n",
                "for g in GROUPBY_OPTIONS:\n",
                "    plot_metric_comparison(comparison_df, 'mse', title='Averaged MSE (lower is better) for each VLM + OpenVLA baseline', groupby=g, filterby=filterby, other_vals={\"openvla\": BASELINE_METRICS['openvla']['mse']})"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "best_any_way_df = comparison_df.T.groupby('model_name').agg('min').sort_values('mse')\n",
                "plt.figure(figsize=(10, 6))\n",
                "xvals = list(best_any_way_df.index)\n",
                "yvals =list(best_any_way_df['mse'])\n",
                "bars = plt.bar(xvals, yvals)\n",
                "plt.axhline(y=BASELINE_METRICS['openvla']['mse'], color='gray', linestyle='--', alpha=0.7, label='openvla')\n",
                "\n",
                "# Add value labels on top of bars\n",
                "for bar in bars:\n",
                "    height = bar.get_height()\n",
                "    plt.text(bar.get_x() + bar.get_width()/2., height + 0.001,\n",
                "            f'{height:.4f}', ha='center', va='bottom', rotation=0, fontsize=8)\n",
                "plt.legend()\n",
                "plt.title('Best MSE (lower is better) for each VLM + OpenVLA Baseline')\n",
                "plt.xlabel('Model')\n",
                "plt.ylabel('MSE')\n",
                "plt.xticks(rotation=45, ha='right')\n",
                "plt.tight_layout()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Action Magnitude Analysis"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Improved action magnitude comparison plot with simple color scheme\n",
                "# Extract data\n",
                "models = list(comparison_metrics.keys())\n",
                "pred_mags = [comparison_metrics[model]['pred_mag'] for model in models]\n",
                "gt_mag = comparison_metrics[models[0]]['gt_mag']  # Ground truth should be the same for all models\n",
                "\n",
                "# Set up a simple color palette\n",
                "colors = plt.cm.tab10(np.arange(len(models)))\n",
                "\n",
                "# Create plot\n",
                "fig, ax = plt.subplots(figsize=(12, 6))\n",
                "bars = ax.bar(np.arange(len(models)), pred_mags, color=colors, alpha=0.8)\n",
                "\n",
                "# Add horizontal line for ground truth\n",
                "ax.axhline(y=gt_mag, color='red', linestyle='--', alpha=0.7, label=f'Ground Truth: {gt_mag:.4f}')\n",
                "\n",
                "# Add value labels on top of bars\n",
                "for i, bar in enumerate(bars):\n",
                "    height = bar.get_height()\n",
                "    ax.text(bar.get_x() + bar.get_width()/2., \n",
                "            height + 0.01,\n",
                "            f'{height:.4f}', \n",
                "            ha='center', \n",
                "            va='bottom',\n",
                "            fontsize=10)\n",
                "\n",
                "# Set labels and title\n",
                "ax.set_xlabel('Model')\n",
                "ax.set_ylabel('Action Magnitude')\n",
                "ax.set_title('Action Magnitude Comparison')\n",
                "ax.set_xticks(np.arange(len(models)))\n",
                "ax.set_xticklabels(models, rotation=45, ha='right')\n",
                "ax.legend()\n",
                "\n",
                "# Add grid for better readability\n",
                "ax.grid(axis='y', linestyle='--', alpha=0.3)\n",
                "\n",
                "plt.tight_layout()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "from __future__ import annotations\n",
                "\n",
                "\"\"\"plot_mse_by_model\n",
                "====================\n",
                "flexible visualisation of mse vs history_length with robust sizing logic.\n",
                "\n",
                "changes vs previous rev\n",
                "-----------------------\n",
                "* `expected_frames may contain **zero** → size fallback of *1 frame* so the dot is visible.\n",
                "* size legend automatically deduplicates values and spans the whole numeric range, incl. `0 when present.\n",
                "* public API unchanged: `plot_mse_by_model(df, grid=False, size_scale=20.0).\n",
                "\n",
                "layout recap\n",
                "------------\n",
                "* one‑axes (default) or N×N grid (`grid=True).\n",
                "* marker → model_name, color → history_choice, size → expected_frames.\n",
                "* tri‑column legend outside right for color, size, marker.\n",
                "\"\"\"\n",
                "\n",
                "from typing import Dict, List\n",
                "\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "from matplotlib.lines import Line2D\n",
                "\n",
                "# ----------------------------- aesthetics helpers -----------------------------\n",
                "MARKERS: list[str] = [\n",
                "    \"o\", \"s\", \"^\", \"D\", \"v\",\n",
                "    \"P\", \"X\", \"*\", \"<\", \">\",\n",
                "    \"h\", \"H\", \"p\", \"8\",\n",
                "]\n",
                "\n",
                "\n",
                "def _build_marker_map(models: list[str]) -> dict[str, str]:\n",
                "    if len(models) > len(MARKERS):\n",
                "        raise ValueError(\"too many models for built‑in marker set; extend MARKERS list\")\n",
                "    return {m: MARKERS[i] for i, m in enumerate(models)}\n",
                "\n",
                "\n",
                "# ----------------------------- size helpers -----------------------------\n",
                "\n",
                "def _effective_size(frames: float | pd.Series, scale: float) -> float | pd.Series:\n",
                "    \"\"\"convert expected_frames → marker size in points²; zero gets minimal dot.\"\"\"\n",
                "    if isinstance(frames, pd.Series):\n",
                "        return frames.clip(lower=1.0) * scale  # 0 → 1\n",
                "    return max(frames, 1.0) * scale  # 0 → 1\n",
                "\n",
                "\n",
                "def _legend_size_values(frames: np.ndarray) -> list[int]:\n",
                "    \"\"\"choose up‑to‑4 representative frame counts for legend.\n",
                "\n",
                "    * always include 0 when present.\n",
                "    * include min, median, max of non‑zero values (deduped).\n",
                "    \"\"\"\n",
                "    frames = frames[~np.isnan(frames)]\n",
                "    zeros_present = np.any(frames == 0)\n",
                "    nonzero = frames[frames > 0]\n",
                "    legend_vals: list[int] = []\n",
                "\n",
                "    if zeros_present:\n",
                "        legend_vals.append(0)\n",
                "\n",
                "    if nonzero.size:\n",
                "        # pick min, median, max unique ints\n",
                "        q_vals = [np.min(nonzero), np.percentile(nonzero, 25), np.median(nonzero), np.percentile(nonzero, 75), np.percentile(nonzero, 90)]\n",
                "        for v in q_vals:\n",
                "            iv = int(round(v))\n",
                "            if iv not in legend_vals:\n",
                "                legend_vals.append(iv)\n",
                "\n",
                "    return sorted(legend_vals, key=lambda x: int(x))\n",
                "\n",
                "\n",
                "# ----------------------------- single‑axes plot -----------------------------\n",
                "\n",
                "def _plot_single(df: pd.DataFrame, marker_map: dict[str, str], cdict: dict[str, str], *, size_scale: float, other_vals: dict[str, float]) -> None:\n",
                "    fig, ax = plt.subplots(figsize=(10, 8))\n",
                "\n",
                "    for _, row in df.iterrows():\n",
                "        ax.scatter(\n",
                "            row[\"history_length\"],\n",
                "            row[\"mse\"],\n",
                "            s=_effective_size(row[\"expected_frames\"], size_scale),\n",
                "            marker=marker_map[row[\"model_name\"]],\n",
                "            color=cdict[row[\"history_choice\"]],\n",
                "            alpha=0.8,\n",
                "        )\n",
                "\n",
                "    ax.set_xlabel(\"history_len (# frames)\")\n",
                "    ax.set_ylabel(\"mse\")\n",
                "    ax.set_title(\"mse vs history_len across models\")\n",
                "    for k,v in other_vals.items():\n",
                "        ax.axhline(y=v, color='gray', linestyle='--', alpha=0.7, label=k)\n",
                "\n",
                "    _add_legends(ax, df, marker_map, cdict, size_scale)\n",
                "    plt.tight_layout()\n",
                "    plt.show()\n",
                "\n",
                "\n",
                "# ----------------------------- grid plot -----------------------------\n",
                "from typing import Dict\n",
                "import math\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "from typing import Dict\n",
                "import math\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "def _plot_grid(  # noqa: N802\n",
                "    df: pd.DataFrame,\n",
                "    marker_map: dict[str, str],\n",
                "    cdict: dict[str, str],\n",
                "    *,\n",
                "    size_scale: float,\n",
                "    sync_axes: bool = True,\n",
                "    other_vals: dict[str, float] = {},\n",
                ") -> None:\n",
                "    extend_pct = 0.1\n",
                "    if sync_axes:\n",
                "        x_min, x_max = df[\"history_length\"].dropna().agg([\"min\", \"max\"])\n",
                "        y_min, y_max = df[\"mse\"].dropna().agg([\"min\", \"max\"])\n",
                "        if other_vals:\n",
                "            y_min = min(y_min, min(other_vals.values()))\n",
                "            y_max = max(y_max, max(other_vals.values()))\n",
                "        x_range = x_max - x_min\n",
                "        x_min, x_max = x_min - (x_range * extend_pct), x_max + (x_range * extend_pct)\n",
                "        y_range = y_max - y_min\n",
                "        y_min, y_max = y_min - (y_range * extend_pct), y_max + (y_range * extend_pct)\n",
                "        sharex = sharey = True\n",
                "    else:\n",
                "        sharex = sharey = False\n",
                "\n",
                "    models = sorted(df[\"model_name\"].unique())\n",
                "    n_mod = len(models)\n",
                "    side = math.ceil(math.sqrt(n_mod))\n",
                "    bottom_row = (n_mod - 1) // side  # last visible row\n",
                "\n",
                "    fig, axes = plt.subplots(\n",
                "        side,\n",
                "        side,\n",
                "        figsize=(6 * side, 6 * side),\n",
                "        sharex=sharex,\n",
                "        sharey=sharey,\n",
                "    )\n",
                "\n",
                "    axes_flat = axes.ravel() if n_mod > 1 else [axes]\n",
                "    for ax in axes_flat[n_mod:]:\n",
                "        ax.set_visible(False)\n",
                "\n",
                "    for idx, (ax, model) in enumerate(zip(axes_flat, models)):\n",
                "        sdf = df[df[\"model_name\"] == model]\n",
                "        for choice, sub in sdf.groupby(\"history_choice\"):\n",
                "            ax.scatter(\n",
                "                sub[\"history_length\"],\n",
                "                sub[\"mse\"],\n",
                "                s=_effective_size(sub[\"expected_frames\"], size_scale).astype(float),\n",
                "                marker=marker_map[model],\n",
                "                color=cdict[choice],\n",
                "                alpha=0.8,\n",
                "                label=str(choice),  # labels grabbed later\n",
                "            )\n",
                "        \n",
                "        for k,v in other_vals.items():\n",
                "            ax.axhline(y=v, color='gray', linestyle='--', alpha=0.7, label=k)\n",
                "\n",
                "        ax.set_title(model)\n",
                "        if sync_axes:\n",
                "            ax.set_xlim(x_min, x_max)\n",
                "            ax.set_ylim(y_min, y_max)\n",
                "\n",
                "        row, col = divmod(idx, side)\n",
                "        if col == 0:\n",
                "            ax.set_ylabel(\"mse\")\n",
                "        \n",
                "        ax.set_xlabel(\"history_len (# frames)\")\n",
                "        ax.tick_params(axis=\"x\", labelbottom=True)   # show xtick labels here\n",
                "\n",
                "    # build combined legend once and shove it outside\n",
                "    handles, labels = axes_flat[0].get_legend_handles_labels()\n",
                "    fig.legend(handles, labels, loc=\"center left\", bbox_to_anchor=(1.02, 0.5))\n",
                "\n",
                "    plt.tight_layout()\n",
                "    plt.subplots_adjust(right=0.82)  # leave room for legend\n",
                "    plt.show()\n",
                "\n",
                "\n",
                "\n",
                "\n",
                "# ----------------------------- legend builder -----------------------------\n",
                "\n",
                "def _add_legends(ax, df: pd.DataFrame, marker_map: dict[str, str], cdict: dict[str, str], size_scale: float) -> None:\n",
                "    choices = sorted(df[\"history_choice\"].unique())\n",
                "\n",
                "    # 1) color legend (history_choice)\n",
                "    color_handles = [\n",
                "        Line2D([], [], marker=\"o\", linestyle=\"\", markerfacecolor=cdict[c], markeredgecolor=\"none\", markersize=10, label=c)\n",
                "        for c in choices\n",
                "    ]\n",
                "\n",
                "    # 2) size legend (expected_frames)\n",
                "    expected_frames = df[\"expected_frames\"]\n",
                "    size_vals = _legend_size_values(expected_frames.astype(float).values)\n",
                "    # size_handles = [\n",
                "    #     ax.scatter([], [], s=_effective_size(v, size_scale), color=\"gray\", alpha=0.6, label=f\"{v} frames\")\n",
                "    #     for v in size_vals\n",
                "    # ]\n",
                "    size_handles = [\n",
                "        Line2D([], [], marker=\"o\", linestyle=\"\", color=\"gray\", markersize=_effective_size(v, size_scale)/10, label=f\"{v} frames\")\n",
                "        for v in size_vals\n",
                "    ]\n",
                "\n",
                "    # 3) marker legend (model_name)\n",
                "    marker_handles = [\n",
                "        Line2D([], [], marker=marker_map[m], linestyle=\"\", color=\"gray\", markersize=10, label=m)\n",
                "        for m in sorted(marker_map)\n",
                "    ]\n",
                "\n",
                "    legend_kw = dict(loc=\"upper left\", bbox_to_anchor=(1.02, 1.1), frameon=False, alignment=\"left\")\n",
                "    first = ax.legend(handles=color_handles, title=\"History Choice\", **legend_kw)\n",
                "    ax.add_artist(first)\n",
                "\n",
                "    legend_kw[\"bbox_to_anchor\"] = (1.02, 0.45)\n",
                "    second = ax.legend(handles=size_handles, title=\"Expected # Frames\", **legend_kw)\n",
                "    ax.add_artist(second)\n",
                "\n",
                "\n",
                "    legend_kw[\"bbox_to_anchor\"] = (1.02, 0.0)\n",
                "    ax.legend(handles=marker_handles, title=\"Model Name\", **legend_kw)\n",
                "    \n",
                "# ----------------------------- public API -----------------------------\n",
                "\n",
                "def plot_mse_by_model(\n",
                "    df: pd.DataFrame,\n",
                "    *,\n",
                "    grid: bool = False,\n",
                "    sync_axes: bool = True,\n",
                "    size_scale: float = 20.0,\n",
                "    other_vals: dict[str, float] = {},\n",
                ") -> None:\n",
                "    \"\"\"plot mse vs history_length.\n",
                "\n",
                "    parameters\n",
                "    ----------\n",
                "    df : pd.DataFrame\n",
                "        requires columns `{\"model_name\", \"history_length\", \"mse\", \"history_choice\", \"expected_frames\"}.\n",
                "    grid : bool, default `False\n",
                "        True → one subplot per model in a square grid.\n",
                "    size_scale : float, default `20.0\n",
                "        multiplier converting frames → marker points²; zero frames mapped to size of *1 frame*.\n",
                "    \"\"\"\n",
                "\n",
                "    required = {\"model_name\", \"history_length\", \"mse\", \"history_choice\", \"expected_frames\"}\n",
                "    if not required.issubset(df.columns):\n",
                "        raise ValueError(f\"df missing columns: {required.difference(df.columns)}\")\n",
                "\n",
                "    models = sorted(df[\"model_name\"].unique())\n",
                "    marker_map = _build_marker_map(models)\n",
                "\n",
                "    choices = sorted(df[\"history_choice\"].unique())\n",
                "    colors = plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]\n",
                "    cdict: dict[str, str] = {c: colors[i % len(colors)] for i, c in enumerate(choices)}\n",
                "\n",
                "    if grid:\n",
                "        _plot_grid(df, marker_map, cdict, sync_axes=sync_axes, size_scale=size_scale, other_vals=other_vals)\n",
                "    else:\n",
                "        _plot_single(df, marker_map, cdict, size_scale=size_scale, other_vals=other_vals)\n",
                "    return cdict\n",
                "\n",
                "\n",
                "color_dict = plot_mse_by_model(comparison_df.T, grid=False, other_vals={\"openvla\": BASELINE_METRICS['openvla']['mse']})"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "comparison_df.T[comparison_df.T['expected_frames'] == 0].history_length.unique()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "plot_mse_by_model(comparison_df.T, grid=True, other_vals={\"openvla\": BASELINE_METRICS['openvla']['mse']})"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "NO_DATA_KEY = \"no data\"\n",
                "IMPOSSIBLE_KEY = \"impossible\"\n",
                "color_dict[NO_DATA_KEY] = '#000000'\n",
                "color_dict[IMPOSSIBLE_KEY] = '#FFFFFF'\n",
                "print(color_dict)\n",
                "\n",
                "color_mapping = {k: j for j, (k, v) in enumerate(color_dict.items())}\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 286,
            "metadata": {},
            "outputs": [],
            "source": [
                "def get_history_choice_heatmap(df, global_expected_frames=None, global_hist_lengths=None):\n",
                "    if global_hist_lengths:\n",
                "        unique_history_lengths = global_hist_lengths\n",
                "    else:\n",
                "        unique_history_lengths = sorted(df.T.history_length.unique())\n",
                "    \n",
                "    if global_expected_frames:\n",
                "        unique_expected_frames = global_expected_frames\n",
                "    else:\n",
                "        unique_expected_frames = sorted(df.T.expected_frames.unique())\n",
                "\n",
                "    # drop odds for cleaner diagram\n",
                "    unique_history_lengths = [x for x in unique_history_lengths if x%2 ==0 or x==1]\n",
                "    unique_expected_frames = [x for x in unique_expected_frames if x%2 ==0 or x==1]\n",
                "    # drop 0 for history focused diagram\n",
                "    unique_history_lengths = [x for x in unique_history_lengths if x!=0]\n",
                "    unique_expected_frames = [x for x in unique_expected_frames if x!=0]\n",
                "    \n",
                "    arr = np.zeros((len(unique_history_lengths), len(unique_expected_frames)), dtype=int)\n",
                "\n",
                "    for i, history_length in enumerate(unique_history_lengths):\n",
                "        for j, expected_frames in enumerate(unique_expected_frames):\n",
                "            if expected_frames > history_length:\n",
                "                arr[i, j] = color_mapping[IMPOSSIBLE_KEY]\n",
                "                continue\n",
                "            this_df = df.T[\n",
                "                (df.T.history_length.astype(int) == history_length) &\n",
                "                (df.T.expected_frames.astype(int) == expected_frames)\n",
                "            ]\n",
                "            if this_df.empty:\n",
                "                arr[i, j] = color_mapping[NO_DATA_KEY]\n",
                "                continue\n",
                "                \n",
                "            this_df.sort_values(by='mse', ascending=True, inplace=True)\n",
                "            best_history_choice = this_df.iloc[0]['history_choice']\n",
                "            arr[i, j] = color_mapping[best_history_choice]\n",
                "    return arr, unique_expected_frames, unique_history_lengths"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 287,
            "metadata": {},
            "outputs": [],
            "source": [
                "history_choice_heatmaps = dict()\n",
                "overall_history_choice_details = get_history_choice_heatmap(comparison_df)\n",
                "history_choice_heatmaps['overall'] = overall_history_choice_details[0]\n",
                "\n",
                "for model_name in comparison_df.T.model_name.unique():\n",
                "    this_df = comparison_df.T[comparison_df.T.model_name.apply(lambda x: model_name in x)].T\n",
                "    history_choice_heatmaps[model_name] = get_history_choice_heatmap(this_df, overall_history_choice_details[1], overall_history_choice_details[2])[0]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 288,
            "metadata": {},
            "outputs": [],
            "source": [
                "from matplotlib.colors import ListedColormap, BoundaryNorm\n",
                "\n",
                "def plot_history_choice_heatmaps(arr_dict, unique_expected_frames, unique_history_lengths, color_dict, color_mapping, overall_title):\n",
                "    n_plots = len(arr_dict)\n",
                "    n_cols = int(np.ceil(np.sqrt(n_plots)))\n",
                "    n_rows = int(np.ceil(n_plots / n_cols))\n",
                "\n",
                "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15,15))\n",
                "    if n_plots > 1:\n",
                "        axes = axes.flatten()\n",
                "    else:\n",
                "        axes = [axes]\n",
                "\n",
                "    colors = list(color_dict.values())\n",
                "    cmap = plt.cm.colors.ListedColormap(colors)\n",
                "    bounds = np.arange(len(color_dict)+1)\n",
                "    norm = BoundaryNorm(bounds, cmap.N)\n",
                "\n",
                "    print(f\"bounds: {bounds}\")\n",
                "\n",
                "    print(f\"colors: {colors}\")\n",
                "    print(f\"cmap: {cmap}\")\n",
                "\n",
                "    for i, (title, arr) in enumerate(arr_dict.items()):\n",
                "        ax = axes[i]\n",
                "        im = ax.imshow(arr, cmap=cmap, norm=norm)\n",
                "        ax.grid(False)\n",
                "        ax.set_xticks(np.arange(len(unique_expected_frames)), unique_expected_frames)\n",
                "        ax.set_yticks(np.arange(len(unique_history_lengths)), unique_history_lengths)\n",
                "        ax.set_title(f\"{title}\", fontsize=14)\n",
                "        ax.set_xlabel(\"Expected Frames\", fontsize=12)\n",
                "        ax.set_ylabel(\"History Length\", fontsize=12)\n",
                "\n",
                "        # Add grid lines to better separate cells\n",
                "        ax.set_xticks(np.arange(len(unique_expected_frames) + 1) - 0.5, minor=True)\n",
                "        ax.set_yticks(np.arange(len(unique_history_lengths) + 1) - 0.5, minor=True)\n",
                "        ax.tick_params(which='minor', length=0)  # Remove the tick marks on grid lines\n",
                "\n",
                "\n",
                "    cbar_ax = fig.add_axes([1.05, 0.15, 0.02, 0.7])  # [left, bottom, width, height]\n",
                "    cbar = fig.colorbar(im, cax=cbar_ax, boundaries=bounds)\n",
                "    \n",
                "    print(f\"color mapping ({len(color_mapping)}): {color_mapping}\")\n",
                "    print(f\"color dict ({len(color_dict)}): {color_dict}\")\n",
                "    \n",
                "    # Create custom colorbar ticks at the center of each color segment\n",
                "    cbar_ticks = np.arange(len(color_mapping)) + 0.5\n",
                "    cbar.set_ticks(cbar_ticks)\n",
                "    cbar_tick_labels = color_dict.keys()\n",
                "    cbar.set_ticklabels(list(cbar_tick_labels))\n",
                "    cbar.set_label('Best History Choice', fontsize=12)\n",
                "    cbar_ax.tick_params(which='minor', length=0)  # Remove the tick marks on grid lines\n",
                "\n",
                "\n",
                "    if overall_title:\n",
                "        fig.suptitle(overall_title)\n",
                "\n",
                "    plt.tight_layout()  # Adjust layout for better spacing\n",
                "    # Hide any unused subplots\n",
                "    for j in range(i + 1, n_rows * n_cols):\n",
                "        if j < len(axes):\n",
                "            axes[j].axis('off')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "overall_title = \"Best History Choice for each combination of History Length and Expected Frames\"\n",
                "plot_history_choice_heatmaps({\"All Runs\": overall_history_choice_details[0]}, overall_history_choice_details[1], overall_history_choice_details[2], color_dict, color_mapping, overall_title=overall_title)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "plot_history_choice_heatmaps(history_choice_heatmaps, overall_history_choice_details[1], overall_history_choice_details[2], color_dict, color_mapping, overall_title=\"Best History Choice per History Length-Expected Frame Combination\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# figure out the missing cells\n",
                "unique_history_choices = overall_history_choice_details[2]\n",
                "unique_expected_frames = overall_history_choice_details[1]\n",
                "unique_history_lengths = comparison_df.T.history_length.unique()\n",
                "unique_model_names = comparison_df.T.model_name.unique()\n",
                "unique_history_choices = comparison_df.T.history_choice.unique()\n",
                "\n",
                "found_rows = comparison_df.T[['history_length', 'history_choice', 'model_name']].drop_duplicates()\n",
                "\n",
                "missing_rows = []\n",
                "for model_name in unique_model_names:\n",
                "    for history_length in unique_history_lengths:\n",
                "        for history_choice in unique_history_choices:\n",
                "            # special checks for history 0 and 1 --> only None and One as history choices\n",
                "            if history_length == 0:\n",
                "                if history_choice != 'None':\n",
                "                    continue\n",
                "            elif history_length == 1:\n",
                "                if history_choice != 'one':\n",
                "                    continue\n",
                "            else:\n",
                "                # for other history lengths, none and one aren't valid options\n",
                "                if history_choice in ['one']:\n",
                "                    continue\n",
                "\n",
                "            # Check if this combination exists in found_rows\n",
                "            query = (\n",
                "                (found_rows['model_name'] == model_name) & \n",
                "                (found_rows['history_length'] == history_length) & \n",
                "                (found_rows['history_choice'] == history_choice)\n",
                "            )\n",
                "            if not query.any():\n",
                "                missing_rows.append((model_name, history_length, history_choice))\n",
                "\n",
                "missing_rows = pd.DataFrame(missing_rows, columns=['model_name', 'history_length', 'history_choice'])\n",
                "missing_rows.sort_values(by=['model_name', 'history_length', 'history_choice'], inplace=True)\n",
                "\n",
                "print(f\"found {len(found_rows)} rows\")\n",
                "print(f\"missing {len(missing_rows)} rows\")\n",
                "\n",
                "for k, v in missing_rows.groupby(\"model_name\"):\n",
                "    print(f\"missing for {k}\")\n",
                "    print(v.head())"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "lapa",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.15"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
