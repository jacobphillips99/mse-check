{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Comparison for Robot Policy Performance\n",
    "\n",
    "This notebook compares the performance of different models tested using the `test_policy_client_mse.py` script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1110,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from typing import Dict, List, Any\n",
    "\n",
    "# Set plot style\n",
    "plt.style.use('ggplot')\n",
    "sns.set_context(\"talk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths to model action files\n",
    "# model_paths = {\n",
    "#     \"openvla\": \"results/actions_openvla_20250421_152240.pkl\",\n",
    "#     \"o4-mini\": \"results/actions_o4-mini_20250421_152435.pkl\",\n",
    "#     \"gpt-4o\": \"results/actions_gpt-4o_20250421_180623.pkl\",\n",
    "#     # \"gpt-4o-tunnel\": \"results/actions_gpt-4o-tunnel_20250423_134722.pkl\",\n",
    "#     # \"gpt-4o-tunnel-async\": \"results/actions_gpt-4o-tunnel_20250423_140019.pkl\",\n",
    "#     # \"gpt-4o-mini\": \"results/actions_gpt-4o-mini_20250423_152620.pkl\",\n",
    "#     # \"gpt-4o-mini-tunnel-async\": \"results/actions_gpt-4o-mini-async_20250423_140435.pkl\",\n",
    "#     # \"gpt-4o-mini-async\": \"results/actions_gpt-4o-mini_20250423_141040.pkl\",\n",
    "#     # \"gemini-2.0-flash-tunnel\": \"results/actions_gemini-2.0-flash-tunnel_20250423_142234.pkl\",\n",
    "#     \"o3\": \"results/actions_o3_20250421_181454.pkl\",\n",
    "#     \"gemini-2.5-pro\": \"results/actions_gemini-2-5-pro_20250421_172434.pkl\",\n",
    "#     \"gemini-2.5-pro-history1\": \"results/actions_gemini-2-5-pro-history1_20250429_151242.pkl\",\n",
    "#     # \"gemini-2.5-flash\": \"results/actions_gemini-2.5-flash_20250421_180103.pkl\",\n",
    "#     # \"gemini-2.5-flash-history1\": \"results/actions_gemini-2-5-flash-history1_20250429_154648.pkl\",\n",
    "#     \"gemini-2.5-pro-historytest\": \"results/actions_testhist_20250501_221849.pkl\",\n",
    "#     \"gemini-2.5-pro-historytest2\": \"results/actions_testhist_20250501_222835.pkl\",\n",
    "#     \"gemini-2-5-pro-historytest3-lang\": \"results/actions_testhist_lang_20250501_230050.pkl\",\n",
    "#     # \"gemini-2.5-flash-tunnel\": \"results/actions_gemini-2.5-flash-tunnel_20250423_133919.pkl\",\n",
    "#     \"gemini-2-0-flash\": \"results/actions_gemini-2-0-flash_20250422_140401.pkl\",\n",
    "#     # \"gemini-2.0-flash-tunnel-async\": \"results/actions_gemini-2.0-flash-tunnel-async_20250423_142053.pkl\",\n",
    "#     \"claude-3-5-sonnet\": \"results/actions_claude-3-5-sonnet_20250422_135539.pkl\",\n",
    "#     \"claude-3-7-sonnet\": \"results/actions_claude-3-7-sonnet_20250422_141227.pkl\",\n",
    "#     # \"openvla-tunnel\": \"results/actions_openvla-tunnel_20250423_110538.pkl\",\n",
    "#     # \"openvla-tunnel2\": \"results/actions_openvla-tunnel2_20250423_110927.pkl\"\n",
    "#     # Add more models as needed\n",
    "#     \"ecot\": \"results/actions_ecot-tunnel_20250424_173038.pkl\"\n",
    "# }\n",
    "\n",
    "def calc_expected_frames(history_length: int, history_choice: str) -> int:\n",
    "    if history_choice == \"all\":\n",
    "        return history_length\n",
    "    elif history_choice == \"last\":\n",
    "        return 1\n",
    "    elif history_choice == \"first\":\n",
    "        return 1\n",
    "    elif history_choice == \"alternate\":\n",
    "        return history_length // 2\n",
    "    elif history_choice == \"third\":\n",
    "        return history_length // 3\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid history_choice: {history_choice}\")\n",
    "\n",
    "RESULTS_DIR = \"results\"\n",
    "RESULTS = dict()\n",
    "for model_name in os.listdir(RESULTS_DIR):\n",
    "    if os.path.isdir(os.path.join(RESULTS_DIR, model_name)):\n",
    "        print(f\"checking model {model_name}\")\n",
    "        if model_name.lstrip(\"history_sweep_\") not in RESULTS:\n",
    "            RESULTS[model_name.lstrip(\"history_sweep_\")] = list()\n",
    "        for filename in os.listdir(os.path.join(RESULTS_DIR, model_name)):\n",
    "            if 'metrics' in filename and filename.endswith('.pkl'):\n",
    "                print(f\"found metrics file {filename}\")\n",
    "                metrics_dict = pickle.load(open(os.path.join(RESULTS_DIR, model_name, filename), \"rb\"))\n",
    "                actions_file = filename.replace('metrics', 'actions')\n",
    "                actions_dict = pickle.load(open(os.path.join(RESULTS_DIR, model_name, actions_file), \"rb\"))\n",
    "                info_dict = {k: metrics_dict[k] for k in ['timestamp', 'model_name', 'history_length', 'history_choice']}\n",
    "                info_dict['model_name'] = model_name.lstrip(\"history_sweep_\")\n",
    "                RESULTS[model_name.lstrip(\"history_sweep_\")].append({\"info\": info_dict, \"actions\": actions_dict, \"metrics\": metrics_dict})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def info_dict_to_title(info_dict):\n",
    "    return f\"{info_dict['model_name']}\" + (f\"-histlen-{info_dict['history_length']}-histchoice-{info_dict['history_choice']}\" if info_dict['history_length'] is not None else f\"\")\n",
    "\n",
    "# Check which models were loaded successfully\n",
    "print(f\"Successfully loaded data for {len(RESULTS)} models and {sum([len(v) for v in RESULTS.values()])} results\")\n",
    "for model_name, results_list in RESULTS.items():\n",
    "    print(model_name)\n",
    "    for result in results_list:\n",
    "        print(f\"- {info_dict_to_title(result['info'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute metrics for models without pre-computed metrics\n",
    "def compute_metrics(pred_actions, gt_actions):\n",
    "    results = {}\n",
    "    \n",
    "    # MSE - Mean Squared Error\n",
    "    results[\"mse\"] = np.mean(np.square(pred_actions - gt_actions))\n",
    "    \n",
    "    # MAE - Mean Absolute Error\n",
    "    results[\"mae\"] = np.mean(np.abs(pred_actions - gt_actions))\n",
    "    \n",
    "    # Normalized MSE - divide by variance of ground truth\n",
    "    gt_var = np.var(gt_actions)\n",
    "    if gt_var > 0:\n",
    "        results[\"nmse\"] = results[\"mse\"] / gt_var\n",
    "    else:\n",
    "        results[\"nmse\"] = float('inf')\n",
    "    \n",
    "    # Action magnitude comparison\n",
    "    results[\"pred_mag\"] = np.mean(np.linalg.norm(pred_actions, axis=1))\n",
    "    results[\"gt_mag\"] = np.mean(np.linalg.norm(gt_actions, axis=1))\n",
    "    results[\"mag_ratio\"] = results[\"pred_mag\"] / results[\"gt_mag\"] if results[\"gt_mag\"] > 0 else float('inf')\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract and organize metrics across models for comparison\n",
    "def extract_comparison_metrics(model_data: dict[str, list[dict]], force_compute: bool = True, min_results: int = 4):\n",
    "    comparison = {}\n",
    "    \n",
    "    for model_name, model_results in model_data.items():\n",
    "        for data in model_results:\n",
    "            model_title = info_dict_to_title(data['info'])\n",
    "            if model_title in comparison:\n",
    "                copies = [x for x in comparison.keys() if model_title == x or (model_title in x and len(x)- 2 == len(model_title))]\n",
    "                model_title = f\"{model_title}-{len(copies)}\"\n",
    "            print(f\"adding {model_title} to comparison\")\n",
    "            # If we have pre-computed metrics, use those\n",
    "            if 'metrics' in data and 'avg_metrics' in data['metrics'] and not force_compute:\n",
    "                comparison[model_title] = data['metrics']['avg_metrics']\n",
    "            else:\n",
    "                # Otherwise, compute metrics from actions\n",
    "                print(f\"Computing metrics for {model_title}...\")\n",
    "                all_results = []\n",
    "                \n",
    "                for traj in data['actions']:\n",
    "                    pred_actions = traj['pred_actions']\n",
    "                    gt_actions = traj['gt_actions']\n",
    "                    \n",
    "                    results = compute_metrics(pred_actions, gt_actions)\n",
    "                    \n",
    "                    # Add timing information if available\n",
    "                    if 'inference_times' in traj:\n",
    "                        results['mean_inference_time'] = np.mean(traj['inference_times'])\n",
    "                    all_results.append(results)\n",
    "                \n",
    "                # Compute average metrics across all trajectories\n",
    "                if len(all_results) < min_results:\n",
    "                    print(f\"Not enough results for {model_title}, skipping {len(all_results)} results\")\n",
    "                    continue\n",
    "                metrics = {k: np.mean([r[k] for r in all_results]) for k in all_results[0]}\n",
    "                # add info to the df\n",
    "                metrics['model_name'] = model_name\n",
    "                metrics['model_title'] = model_title\n",
    "                metrics['history_length'] = data['info']['history_length'] if data['info']['history_length'] is not None else 0\n",
    "                metrics['history_choice'] = data['info']['history_choice'] if data['info']['history_choice'] is not None else 'None'\n",
    "                metrics['expected_frames'] = calc_expected_frames(data['info']['history_length'], data['info']['history_choice']) if data['info']['history_length'] is not None else 0\n",
    "                comparison[model_title] = metrics\n",
    "\n",
    "    return comparison\n",
    "\n",
    "def consolidate_reruns(comparison: dict[str, dict[str, Any]]) -> dict[str, dict[str, Any]]:\n",
    "    averaged_comparison = dict()\n",
    "    multiples = [t for t in comparison.keys() if t.endswith('-1')]\n",
    "    for t in multiples:\n",
    "        base_title = t[:-2]\n",
    "        matched_titles = [x for x in comparison.keys() if x.startswith(base_title) and len(x) <= len(t)]\n",
    "        avg_metrics = dict()\n",
    "        for k in comparison[matched_titles[0]].keys():\n",
    "            if type(comparison[matched_titles[0]][k]) in (str, type(None)):\n",
    "                # string metrics like choice, name, title etc are the same\n",
    "                avg_metrics[k] = comparison[matched_titles[0]][k]\n",
    "            else:\n",
    "                avg_metrics[k] = np.mean([comparison[t][k] for t in matched_titles])\n",
    "\n",
    "        averaged_comparison[base_title + f\"-avg-{len(matched_titles)}\"] = avg_metrics\n",
    "        print(f\"averaged {len(matched_titles)} models for {base_title}\")\n",
    "        for m in matched_titles:\n",
    "            del comparison[m]\n",
    "    comparison.update(averaged_comparison)\n",
    "    return comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract performance metrics for comparison\n",
    "all_comparison_metrics = extract_comparison_metrics(RESULTS)\n",
    "comparison_metrics = consolidate_reruns(all_comparison_metrics)\n",
    "\n",
    "metrics_of_interest = list(comparison_metrics.values())[0].keys()\n",
    "print(metrics_of_interest)\n",
    "comparison_df = pd.DataFrame(index=metrics_of_interest)\n",
    "\n",
    "for model_name, metrics in comparison_metrics.items():\n",
    "    model_values = [metrics.get(metric, np.nan) for metric in metrics_of_interest]\n",
    "    comparison_df[model_name] = model_values\n",
    "\n",
    "# Display the comparison table\n",
    "comparison_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Metric Comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1182,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Optional\n",
    "\n",
    "\n",
    "def plot_metric_comparison(comparison_df: pd.DataFrame, metric_name: str, title: Optional[str]=None, lower_is_better=True, groupby: Optional[str] = None, filterby: Optional[tuple[str, Callable]] = None):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Get the data for this metric\n",
    "    if filterby is not None:\n",
    "        comparison_df = comparison_df.T[comparison_df.T.apply(filterby, axis=1)].T\n",
    "\n",
    "    if groupby is not None:\n",
    "        comparison_df = comparison_df.T.groupby([groupby])[[metric_name]].agg('mean').T\n",
    "\n",
    "    metric_values = comparison_df.loc[metric_name]\n",
    "    \n",
    "    # Sort models by metric value\n",
    "    if lower_is_better:\n",
    "        sorted_models = metric_values.sort_values().index\n",
    "    else:\n",
    "        sorted_models = metric_values.sort_values(ascending=False).index\n",
    "    \n",
    "    # Create bar plot\n",
    "    bars = plt.bar(sorted_models, metric_values[sorted_models])\n",
    "    \n",
    "    # Add value labels on top of bars with smaller font size\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height + 0.001,\n",
    "                f'{height:.4f}', ha='center', va='bottom', rotation=0, fontsize=8)\n",
    "    \n",
    "    # Add title and labels\n",
    "    plt.title(title or f'{metric_name.upper()} Comparison Across Models')\n",
    "    plt.ylabel(metric_name)\n",
    "    if groupby is not None:\n",
    "        plt.xlabel(g)\n",
    "    \n",
    "    # Set rotated tick labels with proper alignment\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1179,
   "metadata": {},
   "outputs": [],
   "source": [
    "GROUPBY_OPTIONS = ['model_name', 'history_length', 'history_choice', 'expected_frames']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot MSE comparison (lower is better)\n",
    "filterby = lambda x: \"claude\" in x['model_name'] and 'first' in x['history_choice']\n",
    "\n",
    "for g in GROUPBY_OPTIONS:\n",
    "    plot_metric_comparison(comparison_df, 'mse', 'Mean Squared Error (lower is better)' + f\", grouped by {g}\", groupby=g, filterby=filterby)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_df.T.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import math\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import typing as t\n",
    "\n",
    "def plot_mse_by_model(df: pd.DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Single fig, subplots in a square(ish) grid.\n",
    "    x = history_length\n",
    "    y = mse\n",
    "    color = history_choice\n",
    "    legend lives outside on the right with handles from all subplots.\n",
    "    \"\"\"\n",
    "    if \"model_name\" not in df.columns:\n",
    "        raise ValueError(\"df needs column 'model_name'\")\n",
    "\n",
    "    models = sorted(df[\"model_name\"].unique())\n",
    "    n_mod = len(models)\n",
    "    side = math.ceil(math.sqrt(n_mod))          # min grid side for square\n",
    "    choices = sorted(df[\"history_choice\"].unique())\n",
    "    palette = plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]\n",
    "    cdict = {c: palette[i % len(palette)] for i, c in enumerate(choices)}\n",
    "\n",
    "    fig, axes = plt.subplots(\n",
    "        nrows=side,\n",
    "        ncols=side,\n",
    "        figsize=(6 * side, 6 * side),\n",
    "        sharex=True,\n",
    "        sharey=False,\n",
    "    )\n",
    "\n",
    "    # flatten for easy indexing; hide unused pads\n",
    "    axes_flat = axes.ravel()\n",
    "    for ax in axes_flat[n_mod:]:\n",
    "        ax.set_visible(False)\n",
    "\n",
    "    # Collect all handles and labels from all subplots\n",
    "    all_handles: t.Dict[str, t.Any] = {}\n",
    "    all_labels: t.Set[str] = set()\n",
    "\n",
    "    for idx, (ax, model) in enumerate(zip(axes_flat, models)):\n",
    "        sdf = df[df[\"model_name\"] == model]\n",
    "        for choice, sub in sdf.groupby(\"history_choice\"):\n",
    "            scatter = ax.scatter(\n",
    "                sub[\"history_length\"],\n",
    "                sub[\"mse\"],\n",
    "                s=sub[\"expected_frames\"].values.astype(float) * 100,\n",
    "                alpha=0.8,\n",
    "                color=cdict[choice],\n",
    "                label=choice,\n",
    "            )\n",
    "            # Store the handle for this choice if we haven't seen it yet\n",
    "            if choice not in all_handles:\n",
    "                all_handles[choice] = scatter\n",
    "                all_labels.add(choice)\n",
    "        \n",
    "        ax.set_title(model)\n",
    "        if idx % side == 0:\n",
    "            ax.set_ylabel(\"mse\")\n",
    "        if idx // side == side - 1:\n",
    "            ax.set_xlabel(\"history_len (# frames)\")\n",
    "\n",
    "    # Create the legend using all unique handles\n",
    "    unique_handles = [all_handles[label] for label in sorted(all_labels)]\n",
    "    unique_labels = sorted(all_labels)\n",
    "    \n",
    "    fig.legend(\n",
    "        unique_handles,\n",
    "        unique_labels,\n",
    "        title=\"history_choice\",\n",
    "        loc=\"center left\",\n",
    "        bbox_to_anchor=(1.02, 0.5),\n",
    "        borderaxespad=0.0,\n",
    "        fontsize=\"x-small\",\n",
    "    )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(right=0.8)   # leave space for legend\n",
    "    plt.show()\n",
    "\n",
    "plot_mse_by_model(comparison_df.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_history_expected_bubble(\n",
    "    df: pd.DataFrame,\n",
    "    size_scale: float = 1000.0,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    single‑panel scatter:\n",
    "        x  = history_length\n",
    "        y  = expected_frames\n",
    "        size = mse * size_scale  (bigger bubble == bigger error)\n",
    "        color = model_name\n",
    "    \"\"\"\n",
    "    required = {\"model_name\", \"history_length\", \"expected_frames\", \"mse\"}\n",
    "    if not required.issubset(df.columns):\n",
    "        missing = required - set(df.columns)\n",
    "        raise ValueError(f\"missing columns: {missing}\")\n",
    "\n",
    "    # fixed color palette keyed by model_name\n",
    "    palette = plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]\n",
    "    models   = sorted(df[\"model_name\"].unique())\n",
    "    cmap     = {m: palette[i % len(palette)] for i, m in enumerate(models)}\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(6, 4))\n",
    "    for model, sub in df.groupby(\"model_name\"):\n",
    "        print(sub['expected_frames'].values)\n",
    "        print(sub['history_length'].values)\n",
    "        print(sub['mse'].values)\n",
    "        \n",
    "        ax.scatter(\n",
    "            sub[\"history_length\"],\n",
    "            sub[\"expected_frames\"],\n",
    "            s=sub[\"mse\"].values.astype(float) * size_scale,\n",
    "            alpha=0.7,\n",
    "            color=cmap[model],\n",
    "            label=model,\n",
    "            edgecolors=\"none\",\n",
    "        )\n",
    "\n",
    "    ax.set_xlabel(\"history_len (# frames)\")\n",
    "    ax.set_ylabel(\"expected_frames\")\n",
    "    ax.set_title(\"history_len vs expected_frames (bubble size = mse)\")\n",
    "    ax.legend(title=\"model_name\", bbox_to_anchor=(1.02, 0.5), loc=\"center left\", fontsize=\"x-small\")\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(right=0.8)\n",
    "    plt.show()\n",
    "\n",
    "plot_history_expected_bubble(comparison_df.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "\"\"\"plot_mse_by_model_single: one‑axes scatter with tri‑partite legend.\n",
    "\n",
    "x              → history_length\n",
    "y              → mse\n",
    "marker (shape) → model_name\n",
    "color          → history_choice\n",
    "size           → expected_frames (scaled)\n",
    "\n",
    "legend columns (all outside right):\n",
    "    history_choice  (color swatches)\n",
    "    expected_frames (size examples)\n",
    "    model_name      (marker shapes)\n",
    "\"\"\"\n",
    "\n",
    "import math\n",
    "from typing import Dict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "\n",
    "MARKERS = [\n",
    "    \"o\", \"s\", \"^\", \"D\", \"v\",\n",
    "    \"P\", \"X\", \"*\", \"<\", \">\",\n",
    "    \"h\", \"H\", \"p\", \"8\",\n",
    "]\n",
    "\n",
    "\n",
    "def _build_marker_map(models: list[str]) -> Dict[str, str]:\n",
    "    if len(models) > len(MARKERS):\n",
    "        raise ValueError(\"too many models for built‑in marker set; extend MARKERS list\")\n",
    "    return {m: MARKERS[i] for i, m in enumerate(models)}\n",
    "\n",
    "\n",
    "def plot_mse_by_model_single(df: pd.DataFrame, *, size_scale: float = 20.0) -> None:\n",
    "    \"\"\"draw scatter plot where mse is plotted against history_length across models.\"\"\"\n",
    "\n",
    "    required = {\"model_name\", \"history_length\", \"mse\", \"history_choice\", \"expected_frames\"}\n",
    "    if not required.issubset(df.columns):\n",
    "        raise ValueError(f\"df missing columns: {required.difference(df.columns)}\")\n",
    "\n",
    "    # encode aesthetics\n",
    "    models = sorted(df[\"model_name\"].unique())\n",
    "    marker_map = _build_marker_map(models)\n",
    "\n",
    "    choices = sorted(df[\"history_choice\"].unique())\n",
    "    colors = plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]\n",
    "    cdict: Dict[str, str] = {c: colors[i % len(colors)] for i, c in enumerate(choices)}\n",
    "\n",
    "    # plot\n",
    "    fig, ax = plt.subplots(figsize=(12,12))\n",
    "    for _, row in df.iterrows():\n",
    "        ax.scatter(\n",
    "            int(row[\"history_length\"]),\n",
    "            row[\"mse\"],\n",
    "            s=row[\"expected_frames\"] * size_scale,\n",
    "            marker=marker_map[row[\"model_name\"]],\n",
    "            color=cdict[row[\"history_choice\"]],\n",
    "            alpha=0.8,\n",
    "        )\n",
    "\n",
    "    ax.set_xlabel(\"history_len (# frames)\")\n",
    "    ax.set_ylabel(\"mse\")\n",
    "    ax.set_title(\"mse vs history_len across models\")\n",
    "\n",
    "    # build legend handles -------------------------------------------------\n",
    "    # 1) color legend (history_choice)\n",
    "    color_handles = [\n",
    "        Line2D([], [], marker=\"o\", linestyle=\"\", markerfacecolor=cdict[c], markeredgecolor=\"none\", markersize=10, label=c)\n",
    "        for c in choices\n",
    "    ]\n",
    "\n",
    "    # 2) size legend (expected_frames quartiles)\n",
    "    q25, q50, q75 = np.quantile(df[\"expected_frames\"], [0.25, 0.5, 0.75])\n",
    "    size_handles = [\n",
    "        ax.scatter([], [], s=q * size_scale, color=\"gray\", alpha=0.6, label=f\"{int(q)} frames\")\n",
    "        for q in (q25, q50, q75)\n",
    "    ]\n",
    "\n",
    "    # 3) marker legend (model_name)\n",
    "    marker_handles = [\n",
    "        Line2D([], [], marker=marker_map[m], linestyle=\"\", color=\"gray\", markersize=10, label=m)\n",
    "        for m in models\n",
    "    ]\n",
    "\n",
    "    # place legends outside right\n",
    "    legend_kw = dict(loc=\"upper left\", bbox_to_anchor=(1.02, 1), frameon=False)\n",
    "    first = ax.legend(handles=color_handles, title=\"history_choice\", **legend_kw)\n",
    "    ax.add_artist(first)\n",
    "\n",
    "    legend_kw[\"bbox_to_anchor\"] = (1.02, 0.5)\n",
    "    second = ax.legend(handles=size_handles, title=\"expected_frames\", **legend_kw)\n",
    "    ax.add_artist(second)\n",
    "\n",
    "    legend_kw[\"bbox_to_anchor\"] = (1.02, 0)\n",
    "    ax.legend(handles=marker_handles, title=\"model_name\", **legend_kw)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_mse_by_model_single(comparison_df.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "\"\"\"plot_mse_by_model\n",
    "====================\n",
    "flexible visualisation of mse vs history_length with robust sizing logic.\n",
    "\n",
    "changes vs previous rev\n",
    "-----------------------\n",
    "* ``expected_frames`` may contain **zero** → size fallback of *1 frame* so the dot is visible.\n",
    "* size legend automatically deduplicates values and spans the whole numeric range, incl. ``0`` when present.\n",
    "* public API unchanged: ``plot_mse_by_model(df, grid=False, size_scale=20.0)``.\n",
    "\n",
    "layout recap\n",
    "------------\n",
    "* one‑axes (default) or N×N grid (``grid=True``).\n",
    "* marker → model_name, color → history_choice, size → expected_frames.\n",
    "* tri‑column legend outside right for color, size, marker.\n",
    "\"\"\"\n",
    "\n",
    "from typing import Dict, List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "# ----------------------------- aesthetics helpers -----------------------------\n",
    "MARKERS: List[str] = [\n",
    "    \"o\", \"s\", \"^\", \"D\", \"v\",\n",
    "    \"P\", \"X\", \"*\", \"<\", \">\",\n",
    "    \"h\", \"H\", \"p\", \"8\",\n",
    "]\n",
    "\n",
    "\n",
    "def _build_marker_map(models: List[str]) -> Dict[str, str]:\n",
    "    if len(models) > len(MARKERS):\n",
    "        raise ValueError(\"too many models for built‑in marker set; extend MARKERS list\")\n",
    "    return {m: MARKERS[i] for i, m in enumerate(models)}\n",
    "\n",
    "\n",
    "# ----------------------------- size helpers -----------------------------\n",
    "\n",
    "def _effective_size(frames: float, scale: float) -> float:\n",
    "    \"\"\"convert expected_frames → marker size in points²; zero gets minimal dot.\"\"\"\n",
    "    return max(frames, 1.0) * scale  # 0 → 1\n",
    "\n",
    "\n",
    "def _legend_size_values(frames: np.ndarray) -> List[int]:\n",
    "    \"\"\"choose up‑to‑4 representative frame counts for legend.\n",
    "\n",
    "    * always include 0 when present.\n",
    "    * include min, median, max of non‑zero values (deduped).\n",
    "    \"\"\"\n",
    "    frames = frames[~np.isnan(frames)]\n",
    "    zeros_present = np.any(frames == 0)\n",
    "    nonzero = frames[frames > 0]\n",
    "    legend_vals: List[int] = []\n",
    "\n",
    "    if zeros_present:\n",
    "        legend_vals.append(0)\n",
    "\n",
    "    if nonzero.size:\n",
    "        # pick min, median, max unique ints\n",
    "        q_vals = [np.min(nonzero), np.median(nonzero), np.max(nonzero)]\n",
    "        for v in q_vals:\n",
    "            iv = int(round(v))\n",
    "            if iv not in legend_vals:\n",
    "                legend_vals.append(iv)\n",
    "\n",
    "    return legend_vals[:4]  # cap to 4 entries\n",
    "\n",
    "\n",
    "# ----------------------------- single‑axes plot -----------------------------\n",
    "\n",
    "def _plot_single(df: pd.DataFrame, marker_map: Dict[str, str], cdict: Dict[str, str], *, size_scale: float) -> None:\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        ax.scatter(\n",
    "            row[\"history_length\"],\n",
    "            row[\"mse\"],\n",
    "            s=_effective_size(row[\"expected_frames\"], size_scale),\n",
    "            marker=marker_map[row[\"model_name\"]],\n",
    "            color=cdict[row[\"history_choice\"]],\n",
    "            alpha=0.8,\n",
    "        )\n",
    "\n",
    "    ax.set_xlabel(\"history_len (# frames)\")\n",
    "    ax.set_ylabel(\"mse\")\n",
    "    ax.set_title(\"mse vs history_len across models\")\n",
    "\n",
    "    _add_legends(ax, df, marker_map, cdict, size_scale)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# ----------------------------- grid plot -----------------------------\n",
    "\n",
    "def _plot_grid(df: pd.DataFrame, marker_map: Dict[str, str], cdict: Dict[str, str], *, size_scale: float) -> None:\n",
    "    import math\n",
    "\n",
    "    models = sorted(df[\"model_name\"].unique())\n",
    "    n_mod = len(models)\n",
    "    side = math.ceil(math.sqrt(n_mod))\n",
    "\n",
    "    fig, axes = plt.subplots(\n",
    "        nrows=side,\n",
    "        ncols=side,\n",
    "        figsize=(6 * side, 6 * side),\n",
    "        sharex=True,\n",
    "        sharey=False,\n",
    "    )\n",
    "\n",
    "    axes_flat = axes.ravel()\n",
    "    for ax in axes_flat[n_mod:]:\n",
    "        ax.set_visible(False)\n",
    "\n",
    "    for idx, (ax, model) in enumerate(zip(axes_flat, models)):\n",
    "        sdf = df[df[\"model_name\"] == model]\n",
    "        for choice, sub in sdf.groupby(\"history_choice\"):\n",
    "            ax.scatter(\n",
    "                sub[\"history_length\"],\n",
    "                sub[\"mse\"],\n",
    "                s=_effective_size(sub[\"expected_frames\"], size_scale),\n",
    "                marker=marker_map[model],\n",
    "                color=cdict[choice],\n",
    "                alpha=0.8,\n",
    "            )\n",
    "        ax.set_title(model)\n",
    "        if idx % side == 0:\n",
    "            ax.set_ylabel(\"mse\")\n",
    "        if idx // side == side - 1:\n",
    "            ax.set_xlabel(\"history_len (# frames)\")\n",
    "\n",
    "    _add_legends(axes_flat[0], df, marker_map, cdict, size_scale)\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(right=0.8)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# ----------------------------- legend builder -----------------------------\n",
    "\n",
    "def _add_legends(ax, df: pd.DataFrame, marker_map: Dict[str, str], cdict: Dict[str, str], size_scale: float) -> None:\n",
    "    choices = sorted(df[\"history_choice\"].unique())\n",
    "\n",
    "    # 1) color legend (history_choice)\n",
    "    color_handles = [\n",
    "        Line2D([], [], marker=\"o\", linestyle=\"\", markerfacecolor=cdict[c], markeredgecolor=\"none\", markersize=10, label=c)\n",
    "        for c in choices\n",
    "    ]\n",
    "\n",
    "    # 2) size legend (expected_frames)\n",
    "    expected_frames = df[\"expected_frames\"]\n",
    "    size_vals = _legend_size_values(expected_frames.astype(float).values)\n",
    "    size_handles = [\n",
    "        ax.scatter([], [], s=_effective_size(v, size_scale), color=\"gray\", alpha=0.6, label=f\"{v} frames\")\n",
    "        for v in size_vals\n",
    "    ]\n",
    "\n",
    "    # 3) marker legend (model_name)\n",
    "    marker_handles = [\n",
    "        Line2D([], [], marker=marker_map[m], linestyle=\"\", color=\"gray\", markersize=10, label=m)\n",
    "        for m in sorted(marker_map)\n",
    "    ]\n",
    "\n",
    "    legend_kw = dict(loc=\"upper left\", bbox_to_anchor=(1.02, 1), frameon=False)\n",
    "    first = ax.legend(handles=color_handles, title=\"history_choice\", **legend_kw)\n",
    "    ax.add_artist(first)\n",
    "\n",
    "    legend_kw[\"bbox_to_anchor\"] = (1.02, 0.5)\n",
    "    second = ax.legend(handles=size_handles, title=\"expected_frames\", **legend_kw)\n",
    "    ax.add_artist(second)\n",
    "\n",
    "    legend_kw[\"bbox_to_anchor\"] = (1.02, 0)\n",
    "    ax.legend(handles=marker_handles, title=\"model_name\", **legend_kw)\n",
    "\n",
    "\n",
    "# ----------------------------- public API -----------------------------\n",
    "\n",
    "def plot_mse_by_model(\n",
    "    df: pd.DataFrame,\n",
    "    *,\n",
    "    grid: bool = False,\n",
    "    size_scale: float = 20.0,\n",
    ") -> None:\n",
    "    \"\"\"plot mse vs history_length.\n",
    "\n",
    "    parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        requires columns ``{\"model_name\", \"history_length\", \"mse\", \"history_choice\", \"expected_frames\"}``.\n",
    "    grid : bool, default ``False``\n",
    "        True → one subplot per model in a square grid.\n",
    "    size_scale : float, default ``20.0``\n",
    "        multiplier converting frames → marker points²; zero frames mapped to size of *1 frame*.\n",
    "    \"\"\"\n",
    "\n",
    "    required = {\"model_name\", \"history_length\", \"mse\", \"history_choice\", \"expected_frames\"}\n",
    "    if not required.issubset(df.columns):\n",
    "        raise ValueError(f\"df missing columns: {required.difference(df.columns)}\")\n",
    "\n",
    "    models = sorted(df[\"model_name\"].unique())\n",
    "    marker_map = _build_marker_map(models)\n",
    "\n",
    "    choices = sorted(df[\"history_choice\"].unique())\n",
    "    colors = plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]\n",
    "    cdict: Dict[str, str] = {c: colors[i % len(colors)] for i, c in enumerate(choices)}\n",
    "\n",
    "    if grid:\n",
    "        _plot_grid(df, marker_map, cdict, size_scale=size_scale)\n",
    "    else:\n",
    "        _plot_single(df, marker_map, cdict, size_scale=size_scale)\n",
    "\n",
    "\n",
    "plot_mse_by_model(comparison_df.T, grid=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_df.T[comparison_df.T['history_length'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_df.T[comparison_df.T['mse'] < .02]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Cosine Similarity comparison (higher is better)\n",
    "# plot_metric_comparison(comparison_df, 'cosine_sim', 'Cosine Similarity (higher is better)', lower_is_better=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Inference Time comparison (lower is better)\n",
    "if 'mean_inference_time' in comparison_df.index:\n",
    "    # Convert to milliseconds for better readability\n",
    "    inference_times_ms = comparison_df.loc['mean_inference_time'] * 1000\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Sort models by inference time\n",
    "    sorted_models = inference_times_ms.sort_values().index\n",
    "    \n",
    "    # Create bar plot\n",
    "    bars = plt.bar(sorted_models, inference_times_ms[sorted_models])\n",
    "    \n",
    "    # Add value labels on top of bars with smaller font size\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height + 0.5,\n",
    "                f'{height:.1f} ms', ha='center', va='bottom', rotation=0, fontsize=8)\n",
    "    \n",
    "    plt.title('Mean Inference Time (lower is better)')\n",
    "    plt.ylabel('Time (ms)')\n",
    "    plt.xlabel('Model')\n",
    "    \n",
    "    # Set rotated tick labels with proper alignment\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    \n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Action Magnitude Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Improved action magnitude comparison plot with simple color scheme\n",
    "# Extract data\n",
    "models = list(comparison_metrics.keys())\n",
    "pred_mags = [comparison_metrics[model]['pred_mag'] for model in models]\n",
    "gt_mag = comparison_metrics[models[0]]['gt_mag']  # Ground truth should be the same for all models\n",
    "\n",
    "# Set up a simple color palette\n",
    "colors = plt.cm.tab10(np.arange(len(models)))\n",
    "\n",
    "# Create plot\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "bars = ax.bar(np.arange(len(models)), pred_mags, color=colors, alpha=0.8)\n",
    "\n",
    "# Add horizontal line for ground truth\n",
    "ax.axhline(y=gt_mag, color='red', linestyle='--', alpha=0.7, label=f'Ground Truth: {gt_mag:.4f}')\n",
    "\n",
    "# Add value labels on top of bars\n",
    "for i, bar in enumerate(bars):\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., \n",
    "            height + 0.01,\n",
    "            f'{height:.4f}', \n",
    "            ha='center', \n",
    "            va='bottom',\n",
    "            fontsize=10)\n",
    "\n",
    "# Set labels and title\n",
    "ax.set_xlabel('Model')\n",
    "ax.set_ylabel('Action Magnitude')\n",
    "ax.set_title('Action Magnitude Comparison')\n",
    "ax.set_xticks(np.arange(len(models)))\n",
    "ax.set_xticklabels(models, rotation=45, ha='right')\n",
    "ax.legend()\n",
    "\n",
    "# Add grid for better readability\n",
    "ax.grid(axis='y', linestyle='--', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trajectory-level Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract trajectory-level metrics for all models\n",
    "def get_trajectory_metrics(model_data, metric_name='mse'):\n",
    "    traj_metrics = {}\n",
    "    \n",
    "    for model_name, data in model_data.items():\n",
    "        # If we have pre-computed metrics, use those\n",
    "        if 'metrics' in data and 'results' in data['metrics']:\n",
    "            traj_metrics[model_name] = [r.get(metric_name, np.nan) for r in data['metrics']['results']]\n",
    "        else:\n",
    "            # Otherwise, compute metrics from actions\n",
    "            print(f\"Computing trajectory metrics for {model_name}...\")\n",
    "            model_traj_metrics = []\n",
    "            \n",
    "            for traj in data['actions']:\n",
    "                pred_actions = traj['pred_actions']\n",
    "                gt_actions = traj['gt_actions']\n",
    "                \n",
    "                if metric_name == 'mse':\n",
    "                    value = np.mean(np.square(pred_actions - gt_actions))\n",
    "                elif metric_name == 'mae':\n",
    "                    value = np.mean(np.abs(pred_actions - gt_actions))\n",
    "                else:\n",
    "                    # Compute the requested metric\n",
    "                    results = compute_metrics(pred_actions, gt_actions)\n",
    "                    value = results.get(metric_name, np.nan)\n",
    "                    \n",
    "                model_traj_metrics.append(value)\n",
    "            \n",
    "            traj_metrics[model_name] = model_traj_metrics\n",
    "    \n",
    "    return traj_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get MSE for each trajectory across models\n",
    "traj_mse = get_trajectory_metrics(model_data, 'mse')\n",
    "\n",
    "# Calculate average and median MSE for each trajectory (across all models)\n",
    "traj_indices = range(len(next(iter(traj_mse.values()))))\n",
    "avg_per_traj = [np.mean([traj_mse[model][i] for model in traj_mse.keys()]) for i in traj_indices]\n",
    "median_per_traj = [np.median([traj_mse[model][i] for model in traj_mse.keys()]) for i in traj_indices]\n",
    "\n",
    "# Plot trajectory-level MSE comparison\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "# Plot individual model lines\n",
    "for model_name, mse_values in traj_mse.items():\n",
    "    plt.plot(traj_indices, mse_values, marker='o', alpha=0.7, linewidth=1, label=model_name)\n",
    "\n",
    "# Plot average and median with thicker lines for emphasis\n",
    "plt.plot(traj_indices, avg_per_traj, 'k-', linewidth=3, label='Average MSE')\n",
    "plt.plot(traj_indices, median_per_traj, 'r--', linewidth=3, label='Median MSE')\n",
    "\n",
    "plt.title('MSE per Trajectory Across Models')\n",
    "plt.xlabel('Trajectory Index')\n",
    "plt.ylabel('MSE')\n",
    "plt.legend(loc='upper left', bbox_to_anchor=(1, 1))\n",
    "plt.grid(True)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create boxplot for comparison of MSE distribution across models\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Prepare data for boxplot\n",
    "box_data = [values for model, values in traj_mse.items()]\n",
    "box_labels = list(traj_mse.keys())\n",
    "\n",
    "plt.boxplot(box_data, labels=box_labels, patch_artist=True)\n",
    "plt.title('Distribution of MSE Across Trajectories')\n",
    "plt.ylabel('MSE')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimension-wise Action Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze MSE by action dimension\n",
    "def compute_dimension_mse(model_data):\n",
    "    dim_mse = {}\n",
    "    \n",
    "    # First, get action dimensions from the first model\n",
    "    first_model = next(iter(model_data.values()))\n",
    "    first_traj = first_model['actions'][0]\n",
    "    action_dims = first_traj['pred_actions'].shape[1]\n",
    "    \n",
    "    for model_name, data in model_data.items():\n",
    "        dim_mse[model_name] = []\n",
    "        \n",
    "        for dim in range(action_dims):\n",
    "            dim_errors = []\n",
    "            \n",
    "            for traj in data['actions']:\n",
    "                pred_dim = traj['pred_actions'][:, dim]\n",
    "                gt_dim = traj['gt_actions'][:, dim]\n",
    "                dim_errors.append(np.mean(np.square(pred_dim - gt_dim)))\n",
    "            \n",
    "            dim_mse[model_name].append(np.mean(dim_errors))\n",
    "    \n",
    "    return dim_mse, action_dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot dimension-wise MSE on separate subplots with improved styling\n",
    "def plot_dimension_mse_subplots(dim_mse, action_dims):\n",
    "    # Determine number of rows and columns for subplots\n",
    "    ncols = 2\n",
    "    nrows = (action_dims + ncols - 1) // ncols  # Ceiling division\n",
    "    \n",
    "    # Create figure with subplots\n",
    "    fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(16, 4*nrows))\n",
    "    axes = axes.flatten() if nrows > 1 or ncols > 1 else [axes]\n",
    "    \n",
    "    # Define a colormap for consistent colors across models\n",
    "    colors = plt.cm.tab10.colors  # Get colors from tab10 colormap\n",
    "    model_names = list(dim_mse.keys())\n",
    "    model_colors = {model: colors[i % len(colors)] for i, model in enumerate(model_names)}\n",
    "    \n",
    "    # Plot each dimension in its own subplot\n",
    "    for dim in range(action_dims):\n",
    "        ax = axes[dim]\n",
    "        \n",
    "        # Extract MSE values for this dimension across all models\n",
    "        mse_values = [dim_mse[model][dim] for model in model_names]\n",
    "        \n",
    "        # Create bar plot for this dimension with custom colors\n",
    "        bars = ax.bar(model_names, mse_values, color=[model_colors[model] for model in model_names])\n",
    "        \n",
    "        # Add value labels on top of bars with better formatting\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            # Format the text based on the magnitude\n",
    "            if height < 0.001:\n",
    "                text = f'{height:.2e}'  # Scientific notation for very small values\n",
    "            elif height < 0.01:\n",
    "                text = f'{height:.5f}'\n",
    "            else:\n",
    "                text = f'{height:.3f}'\n",
    "                \n",
    "            ax.text(bar.get_x() + bar.get_width()/2., height * 1.02,\n",
    "                   text, ha='center', va='bottom', fontsize=10, \n",
    "                   rotation=0, fontweight='bold', color='black')\n",
    "        \n",
    "        # Set title and labels with better styling\n",
    "        ax.set_title(f'Dimension {dim}', fontsize=14, pad=10)\n",
    "        ax.set_ylabel('MSE', fontsize=12)\n",
    "        \n",
    "        # Improve x-tick labels\n",
    "        ax.set_xticklabels(model_names, rotation=30, ha='right', fontsize=10)\n",
    "        \n",
    "        # Improve grid and background\n",
    "        ax.grid(axis='y', linestyle='--', alpha=0.3)\n",
    "        ax.set_axisbelow(True)  # Put grid lines behind bars\n",
    "        \n",
    "        # Add light background color\n",
    "        ax.set_facecolor('#f8f8f8')\n",
    "    \n",
    "    # Hide any unused subplots\n",
    "    for i in range(action_dims, len(axes)):\n",
    "        axes[i].set_visible(False)\n",
    "    \n",
    "    # Add a single legend for all subplots\n",
    "    handles = [plt.Rectangle((0,0), 1, 1, color=model_colors[model]) for model in model_names]\n",
    "    fig.legend(handles, model_names, loc='upper center', bbox_to_anchor=(0.5, 0), \n",
    "              ncol=min(5, len(model_names)), fontsize=12, frameon=True, facecolor='white')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.suptitle('MSE by Action Dimension (Each plot has its own scale)', \n",
    "                y=1.02, fontsize=16, fontweight='bold')\n",
    "    plt.subplots_adjust(top=0.92, bottom=0.15)  # Make room for the legend at the bottom\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Compute dimension-wise MSE\n",
    "dim_mse, action_dims = compute_dimension_mse(model_data)\n",
    "# Use this function to plot the dimension-wise MSE\n",
    "plot_dimension_mse_subplots(dim_mse, action_dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pred_actions = {k: [tj['pred_actions'] for tj in v['actions']] for k,v in model_data.items()}\n",
    "gt_actions = {k: [tj['gt_actions'] for tj in v['actions']] for k,v in model_data.items()}\n",
    "\n",
    "pred_actions = {k: np.concatenate(v, axis=0 ) for k,v in pred_actions.items()}\n",
    "gt_actions = {k: np.concatenate(v, axis=0) for k,v in gt_actions.items()}\n",
    "\n",
    "for k in pred_actions.keys():\n",
    "    print(f\"pred: {k}, {pred_actions[k].shape}\")\n",
    "    print(f\"gt: {k}, {gt_actions[k].shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_action_dimensions(pred_actions, gt_actions, model_names=None):\n",
    "    \"\"\"\n",
    "    Plot each dimension of the action data over time steps.\n",
    "    \n",
    "    Args:\n",
    "        pred_actions: Dictionary mapping model names to predicted action arrays\n",
    "        gt_actions: Dictionary mapping model names to ground truth action arrays\n",
    "        model_names: Optional list of model names to plot (if None, plots all)\n",
    "    \"\"\"\n",
    "    if model_names is None:\n",
    "        model_names = list(pred_actions.keys())\n",
    "    \n",
    "    # Get the action dimensions from the first model\n",
    "    first_model = next(iter(pred_actions.values()))\n",
    "    action_dims = first_model.shape[1]\n",
    "    \n",
    "    # Create a figure for each action dimension\n",
    "    for dim in range(action_dims):\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        \n",
    "        # Plot predicted actions for each model\n",
    "        for model in model_names:\n",
    "            # Get time steps for x-axis\n",
    "            time_steps = np.arange(len(pred_actions[model]))\n",
    "            \n",
    "            # Plot predicted actions\n",
    "            plt.plot(time_steps, pred_actions[model][:, dim], \n",
    "                     label=f\"{model} (pred)\", alpha=0.7)\n",
    "            \n",
    "            # Plot ground truth actions (only once since it's the same for all models)\n",
    "            if model == model_names[0]:\n",
    "                plt.plot(time_steps, gt_actions[model][:, dim], \n",
    "                         label=\"Ground Truth\", color='black', linestyle='--', linewidth=2)\n",
    "        \n",
    "        plt.title(f\"Action Dimension {dim}\")\n",
    "        plt.xlabel(\"Time Step\")\n",
    "        plt.ylabel(\"Action Value\")\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "plot_action_dimensions(pred_actions, gt_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics_vs_latency(comparison_df, metrics=None, figsize=(14, 10), save_path=None):\n",
    "    \"\"\"\n",
    "    Create scatterplots of metrics vs latency\n",
    "    \n",
    "    Args:\n",
    "        comparison_df: DataFrame with models as columns and metrics as rows\n",
    "        metrics: List of metrics to plot against latency (default: ['mse'])\n",
    "        figsize: Size of the figure (default: (14, 10))\n",
    "        save_path: Path to save the plot (default: None)\n",
    "    \"\"\"\n",
    "    if metrics is None:\n",
    "        metrics = ['mse']\n",
    "    \n",
    "    # Transpose to get models as rows\n",
    "    df_transposed = comparison_df.T\n",
    "    \n",
    "    # Convert latency to milliseconds\n",
    "    df_transposed['latency_ms'] = df_transposed['mean_inference_time'] * 1000\n",
    "    \n",
    "    # Set up the plot grid\n",
    "    n_metrics = len(metrics)\n",
    "    n_cols = min(2, n_metrics)\n",
    "    n_rows = (n_metrics + n_cols - 1) // n_cols  # Ceiling division\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=figsize, squeeze=False)\n",
    "    sns.set(style=\"whitegrid\")\n",
    "    \n",
    "    # Flatten the axes array for easier indexing\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    # Get unique colors for each model\n",
    "    colors = sns.color_palette(\"husl\", len(df_transposed))\n",
    "    \n",
    "    # Plot each metric\n",
    "    for i, metric in enumerate(metrics):\n",
    "        if i < len(axes):\n",
    "            ax = axes[i]\n",
    "            \n",
    "            # Check if the metric exists in the DataFrame\n",
    "            if metric not in df_transposed.columns:\n",
    "                ax.text(0.5, 0.5, f\"Metric '{metric}' not found\", \n",
    "                        ha='center', va='center', transform=ax.transAxes)\n",
    "                continue\n",
    "            \n",
    "            # Plot the points\n",
    "            for j, (model, row) in enumerate(df_transposed.iterrows()):\n",
    "                ax.scatter(\n",
    "                    row['latency_ms'], \n",
    "                    row[metric], \n",
    "                    s=100,  # marker size\n",
    "                    color=colors[j],\n",
    "                    label=model,\n",
    "                    alpha=0.7\n",
    "                )\n",
    "                \n",
    "                # Add model name as text label\n",
    "                ax.text(\n",
    "                    row['latency_ms'] * 1.03,  # slightly offset\n",
    "                    row[metric] * 1.03,  # slightly offset\n",
    "                    model,\n",
    "                    fontsize=9\n",
    "                )\n",
    "            \n",
    "            # Add title and labels\n",
    "            ax.set_title(f'{metric.upper()} vs Latency', fontsize=14)\n",
    "            ax.set_xlabel('Latency (ms)', fontsize=12)\n",
    "            ax.set_ylabel(metric.upper(), fontsize=12)\n",
    "            \n",
    "            # Rotate x-axis tick labels to prevent overlap\n",
    "            plt.setp(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "            \n",
    "            # Add grid\n",
    "            ax.grid(True, linestyle='--', alpha=0.7)\n",
    "            \n",
    "            # Annotate the ideal region (low metric, low latency)\n",
    "            min_latency = df_transposed['latency_ms'].min()\n",
    "            min_metric = df_transposed[metric].min()\n",
    "            \n",
    "            ax.annotate('Better', \n",
    "                       xy=(min_latency, min_metric), \n",
    "                       xytext=(min_latency + 5, min_metric * 0.7),\n",
    "                       arrowprops=dict(facecolor='green', shrink=0.05, width=2),\n",
    "                       fontsize=12, color='green')\n",
    "    \n",
    "    # Hide unused subplots\n",
    "    for j in range(i+1, len(axes)):\n",
    "        axes[j].axis('off')\n",
    "    \n",
    "    # Adjust layout with extra bottom margin for rotated labels\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.97])\n",
    "    \n",
    "    # Save if requested\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"Plot saved as '{save_path}'\")\n",
    "    \n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics_vs_latency(comparison_df, metrics=['mse', 'mae', 'r2', 'cosine_sim'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lapa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
