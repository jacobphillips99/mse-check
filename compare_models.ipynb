{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Model Comparison for Robot Policy Performance\n",
                "\n",
                "This notebook compares the performance of different models tested using the `test_policy_client_mse.py` script.\n",
                "\n",
                "We load all model results from the `results/` dir and concatenate them into a dataframe. Repeats of the same title (such as f\"{model_name}_{history_length}_{history_choice})\" are averaged into one result for the downstream metrics."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 197,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import pickle\n",
                "import os\n",
                "import pandas as pd\n",
                "import seaborn as sns\n",
                "from typing import Dict, List, Any\n",
                "\n",
                "# Set plot style\n",
                "plt.style.use('ggplot')\n",
                "sns.set_context(\"talk\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 198,
            "metadata": {},
            "outputs": [],
            "source": [
                "model_path = \"results/metrics_openvla_20250421_152240.pkl\"\n",
                "with open(model_path, \"rb\") as f:\n",
                "    openvla_data = pickle.load(f)\n",
                "\n",
                "\n",
                "model_path = \"results/metrics_ecot-tunnel_20250424_173038.pkl\"\n",
                "with open(model_path, \"rb\") as f:\n",
                "    ecot_data = pickle.load(f)\n",
                "\n",
                "BASELINE_METRICS = dict()\n",
                "BASELINE_METRICS['openvla'] = openvla_data['avg_metrics']\n",
                "BASELINE_METRICS['ecot'] = ecot_data['avg_metrics']\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def calc_expected_frames(history_length: int, history_choice: str) -> int:\n",
                "    if history_choice == \"all\":\n",
                "        return history_length\n",
                "    elif history_choice == \"last\":\n",
                "        return 1\n",
                "    elif history_choice == \"first\":\n",
                "        return 1\n",
                "    elif history_choice == \"alternate\":\n",
                "        return history_length // 2\n",
                "    elif history_choice == \"third\":\n",
                "        return history_length // 3\n",
                "    elif history_choice.lower() == \"none\":\n",
                "        return 0\n",
                "    elif history_choice.lower() == \"one\":\n",
                "        return 1\n",
                "    else:\n",
                "        raise ValueError(f\"Invalid history_choice: {history_choice}\")\n",
                "\n",
                "RESULTS_DIR = \"results\"\n",
                "RESULTS = dict()\n",
                "for model_name in os.listdir(RESULTS_DIR):\n",
                "    if os.path.isdir(os.path.join(RESULTS_DIR, model_name)):\n",
                "        if model_name.lstrip(\"history_sweep_\") not in RESULTS:\n",
                "            RESULTS[model_name.lstrip(\"history_sweep_\")] = list()\n",
                "        for filename in os.listdir(os.path.join(RESULTS_DIR, model_name)):\n",
                "            if 'metrics' in filename and filename.endswith('.pkl'):\n",
                "                print(f\"found metrics file {filename}\")\n",
                "                metrics_dict = pickle.load(open(os.path.join(RESULTS_DIR, model_name, filename), \"rb\"))\n",
                "                actions_file = filename.replace('metrics', 'actions')\n",
                "                actions_dict = pickle.load(open(os.path.join(RESULTS_DIR, model_name, actions_file), \"rb\"))\n",
                "                info_dict = {k: metrics_dict[k] for k in ['timestamp', 'model_name', 'history_length', 'history_choice']}\n",
                "                info_dict['model_name'] = model_name.lstrip(\"history_sweep_\")\n",
                "                RESULTS[model_name.lstrip(\"history_sweep_\")].append({\"info\": info_dict, \"actions\": actions_dict, \"metrics\": metrics_dict})\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def info_dict_to_title(info_dict):\n",
                "    return f\"{info_dict['model_name']}\" + (f\"-histlen-{info_dict['history_length']}-histchoice-{info_dict['history_choice']}\" if info_dict['history_length'] is not None else f\"\")\n",
                "\n",
                "# Check which models were loaded successfully\n",
                "print(f\"Successfully loaded data for {len(RESULTS)} models and {sum([len(v) for v in RESULTS.values()])} results\")\n",
                "for model_name, results_list in RESULTS.items():\n",
                "    print(model_name)\n",
                "    for result in results_list:\n",
                "        print(f\"- {info_dict_to_title(result['info'])}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 201,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Function to compute metrics for models without pre-computed metrics\n",
                "def compute_metrics(pred_actions, gt_actions):\n",
                "    results = {}\n",
                "    \n",
                "    # MSE - Mean Squared Error\n",
                "    results[\"mse\"] = np.mean(np.square(pred_actions - gt_actions))\n",
                "    \n",
                "    # MAE - Mean Absolute Error\n",
                "    results[\"mae\"] = np.mean(np.abs(pred_actions - gt_actions))\n",
                "    \n",
                "    # Normalized MSE - divide by variance of ground truth\n",
                "    gt_var = np.var(gt_actions)\n",
                "    if gt_var > 0:\n",
                "        results[\"nmse\"] = results[\"mse\"] / gt_var\n",
                "    else:\n",
                "        results[\"nmse\"] = float('inf')\n",
                "    \n",
                "    # Action magnitude comparison\n",
                "    results[\"pred_mag\"] = np.mean(np.linalg.norm(pred_actions, axis=1))\n",
                "    results[\"gt_mag\"] = np.mean(np.linalg.norm(gt_actions, axis=1))\n",
                "    results[\"mag_ratio\"] = results[\"pred_mag\"] / results[\"gt_mag\"] if results[\"gt_mag\"] > 0 else float('inf')\n",
                "    \n",
                "    return results"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 202,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Extract and organize metrics across models for comparison\n",
                "def extract_comparison_metrics(model_data: dict[str, list[dict]], force_compute: bool = True, min_results: int = 4):\n",
                "    comparison = {}\n",
                "    \n",
                "    for model_name, model_results in model_data.items():\n",
                "        for data in model_results:\n",
                "            if data['info']['history_length'] is None:\n",
                "                data['info']['history_length'] = 0 \n",
                "            hist_len = data['info']['history_length']\n",
                "\n",
                "            if hist_len in (None, 0) or data['info']['history_choice'] is None:\n",
                "                data['info']['history_choice'] = 'None'\n",
                "            elif hist_len == 1:\n",
                "                data['info']['history_choice'] = 'one'\n",
                "            hist_choice = data['info']['history_choice']\n",
                "\n",
                "            model_title = info_dict_to_title(data['info'])\n",
                "            if model_title in comparison:\n",
                "                copies = [x for x in comparison.keys() if model_title == x or (model_title in x and len(x)- 2 == len(model_title))]\n",
                "                model_title = f\"{model_title}-{len(copies)}\"\n",
                "            # If we have pre-computed metrics, use those\n",
                "            if 'metrics' in data and 'avg_metrics' in data['metrics'] and not force_compute:\n",
                "                comparison[model_title] = data['metrics']['avg_metrics']\n",
                "            else:\n",
                "                # Otherwise, compute metrics from actions\n",
                "                all_results = []\n",
                "                \n",
                "                for traj in data['actions']:\n",
                "                    pred_actions = traj['pred_actions']\n",
                "                    gt_actions = traj['gt_actions']\n",
                "                    \n",
                "                    results = compute_metrics(pred_actions, gt_actions)\n",
                "                    \n",
                "                    # Add timing information if available\n",
                "                    if 'inference_times' in traj:\n",
                "                        results['mean_inference_time'] = np.mean(traj['inference_times'])\n",
                "                    all_results.append(results)\n",
                "                \n",
                "                # Compute average metrics across all trajectories\n",
                "                if len(all_results) < min_results:\n",
                "                    print(f\"Not enough results for {model_title}, skipping {len(all_results)} results\")\n",
                "                    continue\n",
                "                metrics = {k: np.mean([r[k] for r in all_results]) for k in all_results[0]}\n",
                "                # add info to the df\n",
                "                metrics['model_name'] = model_name\n",
                "                metrics['model_title'] = model_title\n",
                "                metrics['history_length'] = hist_len\n",
                "                metrics['history_choice'] = hist_choice\n",
                "                metrics['expected_frames'] = calc_expected_frames(hist_len, hist_choice) \n",
                "                comparison[model_title] = metrics\n",
                "\n",
                "    return comparison\n",
                "\n",
                "def consolidate_reruns(comparison: dict[str, dict[str, Any]]) -> dict[str, dict[str, Any]]:\n",
                "    averaged_comparison = dict()\n",
                "    multiples = [t for t in comparison.keys() if t.endswith('-1')]\n",
                "    for t in multiples:\n",
                "        base_title = t[:-2]\n",
                "        matched_titles = [x for x in comparison.keys() if x.startswith(base_title) and len(x) <= len(t)]\n",
                "        avg_metrics = dict()\n",
                "        for k in comparison[matched_titles[0]].keys():\n",
                "            if type(comparison[matched_titles[0]][k]) in (str, type(None)):\n",
                "                # string metrics like choice, name, title etc are the same\n",
                "                avg_metrics[k] = comparison[matched_titles[0]][k]\n",
                "            else:\n",
                "                avg_metrics[k] = np.mean([comparison[t][k] for t in matched_titles])\n",
                "\n",
                "        averaged_comparison[base_title + f\"-avg-{len(matched_titles)}\"] = avg_metrics\n",
                "        print(f\"averaged {len(matched_titles)} models for {base_title}\")\n",
                "        for m in matched_titles:\n",
                "            del comparison[m]\n",
                "    comparison.update(averaged_comparison)\n",
                "    return comparison"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Extract performance metrics for comparison\n",
                "all_comparison_metrics = extract_comparison_metrics(RESULTS)\n",
                "comparison_metrics = consolidate_reruns(all_comparison_metrics)\n",
                "\n",
                "metrics_of_interest = list(comparison_metrics.values())[0].keys()\n",
                "print(metrics_of_interest)\n",
                "\n",
                "raw_comparison_df = pd.DataFrame(index=metrics_of_interest)\n",
                "for model_name, metrics in all_comparison_metrics.items():\n",
                "    model_values = [metrics.get(metric, np.nan) for metric in metrics_of_interest]\n",
                "    raw_comparison_df[model_name] = model_values\n",
                "\n",
                "\n",
                "comparison_df = pd.DataFrame(index=metrics_of_interest)\n",
                "\n",
                "for model_name, metrics in comparison_metrics.items():\n",
                "    model_values = [metrics.get(metric, np.nan) for metric in metrics_of_interest]\n",
                "    comparison_df[model_name] = model_values\n",
                "\n",
                "# i messed up naming o4-mini\n",
                "comparison_df.loc['model_name'] = comparison_df.loc['model_name'].str.replace('4-mini', 'o4-mini')\n",
                "\n",
                "# Display the comparison table\n",
                "comparison_df"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "model_name_counts = raw_comparison_df.T.model_name.value_counts()\n",
                "print(model_name_counts)\n",
                "print(f\"Total unique models: {len(model_name_counts)}\\n\")\n",
                "\n",
                "model_title_counts =raw_comparison_df.T.model_title.value_counts()\n",
                "print(f\"Total unique configurations: {len(model_title_counts)}\\n\")\n",
                "\n",
                "histlen_counts = raw_comparison_df.T.history_length.value_counts()\n",
                "print(histlen_counts)\n",
                "\n",
                "histchoice_counts = raw_comparison_df.T.history_choice.value_counts()\n",
                "print(histchoice_counts)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(comparison_df.T.model_name.value_counts())\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Performance Metric Comparisons"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 207,
            "metadata": {},
            "outputs": [],
            "source": [
                "from typing import Callable, Optional\n",
                "\n",
                "from typing import Callable, Optional, Tuple\n",
                "\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "from typing import Callable, Optional\n",
                "\n",
                "\n",
                "\n",
                "def plot_metric_comparison(comparison_df: pd.DataFrame, metric_name: str, title: Optional[str]=None, lower_is_better=True, groupby: Optional[str] = None, filterby: Optional[tuple[str, Callable]] = None, other_vals: Optional[dict[str, float]] = None):\n",
                "    plt.figure(figsize=(10, 6))\n",
                "    \n",
                "    # Get the data for this metric\n",
                "    if filterby is not None:\n",
                "        comparison_df = comparison_df.T[comparison_df.T.apply(filterby, axis=1)].T\n",
                "\n",
                "    if groupby is not None:\n",
                "        comparison_df = comparison_df.T.groupby([groupby])[[metric_name]].agg('mean').T\n",
                "\n",
                "    metric_values = comparison_df.loc[metric_name]\n",
                "    \n",
                "    # Sort models by metric value\n",
                "    if lower_is_better:\n",
                "        sorted_models = metric_values.sort_values().index\n",
                "    else:\n",
                "        sorted_models = metric_values.sort_values(ascending=False).index\n",
                "    \n",
                "    # Create bar plot\n",
                "    bars = plt.bar(sorted_models, metric_values[sorted_models])\n",
                "    \n",
                "    # Add value labels on top of bars with smaller font size\n",
                "    for bar in bars:\n",
                "        height = bar.get_height()\n",
                "        plt.text(bar.get_x() + bar.get_width()/2., height + 0.001,\n",
                "                f'{height:.4f}', ha='center', va='bottom', rotation=0, fontsize=8)\n",
                "    \n",
                "    # Add title and labels\n",
                "    plt.title(title or f'{metric_name.upper()} Comparison Across Models')\n",
                "    plt.ylabel(metric_name)\n",
                "    if groupby is not None:\n",
                "        plt.xlabel(groupby.replace('_', ' ').title())\n",
                "    \n",
                "    # Set rotated tick labels with proper alignment\n",
                "    plt.xticks(rotation=45, ha='right')\n",
                "\n",
                "    if other_vals is not None:\n",
                "        for k,v in other_vals.items():\n",
                "            plt.axhline(y=v, color='gray', linestyle='--', alpha=0.7, label=k)\n",
                "    plt.legend()\n",
                "\n",
                "    plt.tight_layout()\n",
                "    \n",
                "    return plt"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 208,
            "metadata": {},
            "outputs": [],
            "source": [
                "GROUPBY_OPTIONS = ['model_name', 'history_length', 'history_choice', 'expected_frames']"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Plot MSE comparison (lower is better)\n",
                "# filterby = lambda x: \"claude\" in x['model_name'] and 'first' in x['history_choice']\n",
                "filterby = None\n",
                "groupby = \"model_name\"\n",
                "\n",
                "for g in GROUPBY_OPTIONS:\n",
                "    plot_metric_comparison(comparison_df, 'mse', title='Averaged MSE (lower is better) for each VLM + OpenVLA baseline', groupby=g, filterby=filterby, other_vals={\"openvla\": BASELINE_METRICS['openvla']['mse']})"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "best_any_way_df = comparison_df.T.groupby('model_name').agg('min').sort_values('mse')\n",
                "plt.figure(figsize=(10, 6))\n",
                "xvals = list(best_any_way_df.index)\n",
                "yvals =list(best_any_way_df['mse'])\n",
                "bars = plt.bar(xvals, yvals)\n",
                "plt.axhline(y=BASELINE_METRICS['openvla']['mse'], color='gray', linestyle='--', alpha=0.7, label='openvla')\n",
                "\n",
                "# Add value labels on top of bars\n",
                "for bar in bars:\n",
                "    height = bar.get_height()\n",
                "    plt.text(bar.get_x() + bar.get_width()/2., height + 0.001,\n",
                "            f'{height:.4f}', ha='center', va='bottom', rotation=0, fontsize=8)\n",
                "plt.legend()\n",
                "plt.title('Best MSE (lower is better) for each VLM + OpenVLA Baseline')\n",
                "plt.xlabel('Model')\n",
                "plt.ylabel('MSE')\n",
                "plt.xticks(rotation=45, ha='right')\n",
                "plt.tight_layout()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Action Magnitude Analysis"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Improved action magnitude comparison plot with simple color scheme\n",
                "# Extract data\n",
                "models = list(comparison_metrics.keys())\n",
                "pred_mags = [comparison_metrics[model]['pred_mag'] for model in models]\n",
                "gt_mag = comparison_metrics[models[0]]['gt_mag']  # Ground truth should be the same for all models\n",
                "\n",
                "# Set up a simple color palette\n",
                "colors = plt.cm.tab10(np.arange(len(models)))\n",
                "\n",
                "# Create plot\n",
                "fig, ax = plt.subplots(figsize=(12, 6))\n",
                "bars = ax.bar(np.arange(len(models)), pred_mags, color=colors, alpha=0.8)\n",
                "\n",
                "# Add horizontal line for ground truth\n",
                "ax.axhline(y=gt_mag, color='red', linestyle='--', alpha=0.7, label=f'Ground Truth: {gt_mag:.4f}')\n",
                "\n",
                "# Add value labels on top of bars\n",
                "for i, bar in enumerate(bars):\n",
                "    height = bar.get_height()\n",
                "    ax.text(bar.get_x() + bar.get_width()/2., \n",
                "            height + 0.01,\n",
                "            f'{height:.4f}', \n",
                "            ha='center', \n",
                "            va='bottom',\n",
                "            fontsize=10)\n",
                "\n",
                "# Set labels and title\n",
                "ax.set_xlabel('Model')\n",
                "ax.set_ylabel('Action Magnitude')\n",
                "ax.set_title('Action Magnitude Comparison')\n",
                "ax.set_xticks(np.arange(len(models)))\n",
                "ax.set_xticklabels(models, rotation=45, ha='right')\n",
                "ax.legend()\n",
                "\n",
                "# Add grid for better readability\n",
                "ax.grid(axis='y', linestyle='--', alpha=0.3)\n",
                "\n",
                "plt.tight_layout()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "from __future__ import annotations\n",
                "\n",
                "\"\"\"plot_mse_by_model\n",
                "====================\n",
                "flexible visualisation of mse vs history_length with robust sizing logic.\n",
                "\n",
                "changes vs previous rev\n",
                "-----------------------\n",
                "* `expected_frames may contain **zero** → size fallback of *1 frame* so the dot is visible.\n",
                "* size legend automatically deduplicates values and spans the whole numeric range, incl. `0 when present.\n",
                "* public API unchanged: `plot_mse_by_model(df, grid=False, size_scale=20.0).\n",
                "\n",
                "layout recap\n",
                "------------\n",
                "* one‑axes (default) or N×N grid (`grid=True).\n",
                "* marker → model_name, color → history_choice, size → expected_frames.\n",
                "* tri‑column legend outside right for color, size, marker.\n",
                "\"\"\"\n",
                "\n",
                "from typing import Dict, List\n",
                "\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "from matplotlib.lines import Line2D\n",
                "\n",
                "# ----------------------------- aesthetics helpers -----------------------------\n",
                "MARKERS: list[str] = [\n",
                "    \"o\", \"s\", \"^\", \"D\", \"v\",\n",
                "    \"P\", \"X\", \"*\", \"<\", \">\",\n",
                "    \"h\", \"H\", \"p\", \"8\",\n",
                "]\n",
                "\n",
                "\n",
                "def _build_marker_map(models: list[str]) -> dict[str, str]:\n",
                "    if len(models) > len(MARKERS):\n",
                "        raise ValueError(\"too many models for built‑in marker set; extend MARKERS list\")\n",
                "    return {m: MARKERS[i] for i, m in enumerate(models)}\n",
                "\n",
                "\n",
                "# ----------------------------- size helpers -----------------------------\n",
                "\n",
                "def _effective_size(frames: float | pd.Series, scale: float) -> float | pd.Series:\n",
                "    \"\"\"convert expected_frames → marker size in points²; zero gets minimal dot.\"\"\"\n",
                "    if isinstance(frames, pd.Series):\n",
                "        return frames.clip(lower=1.0) * scale  # 0 → 1\n",
                "    return max(frames, 1.0) * scale  # 0 → 1\n",
                "\n",
                "\n",
                "def _legend_size_values(frames: np.ndarray) -> list[int]:\n",
                "    \"\"\"choose up‑to‑4 representative frame counts for legend.\n",
                "\n",
                "    * always include 0 when present.\n",
                "    * include min, median, max of non‑zero values (deduped).\n",
                "    \"\"\"\n",
                "    frames = frames[~np.isnan(frames)]\n",
                "    zeros_present = np.any(frames == 0)\n",
                "    nonzero = frames[frames > 0]\n",
                "    legend_vals: list[int] = []\n",
                "\n",
                "    if zeros_present:\n",
                "        legend_vals.append(0)\n",
                "\n",
                "    if nonzero.size:\n",
                "        # pick min, median, max unique ints\n",
                "        q_vals = [np.min(nonzero), np.percentile(nonzero, 25), np.median(nonzero), np.percentile(nonzero, 75), np.percentile(nonzero, 90)]\n",
                "        for v in q_vals:\n",
                "            iv = int(round(v))\n",
                "            if iv not in legend_vals:\n",
                "                legend_vals.append(iv)\n",
                "\n",
                "    return sorted(legend_vals, key=lambda x: int(x))\n",
                "\n",
                "\n",
                "# ----------------------------- single‑axes plot -----------------------------\n",
                "\n",
                "def _plot_single(df: pd.DataFrame, marker_map: dict[str, str], cdict: dict[str, str], *, size_scale: float, other_vals: dict[str, float]) -> None:\n",
                "    fig, ax = plt.subplots(figsize=(10, 8))\n",
                "\n",
                "    for _, row in df.iterrows():\n",
                "        ax.scatter(\n",
                "            row[\"history_length\"],\n",
                "            row[\"mse\"],\n",
                "            s=_effective_size(row[\"expected_frames\"], size_scale),\n",
                "            marker=marker_map[row[\"model_name\"]],\n",
                "            color=cdict[row[\"history_choice\"]],\n",
                "            alpha=0.8,\n",
                "        )\n",
                "\n",
                "    ax.set_xlabel(\"history_len (# frames)\")\n",
                "    ax.set_ylabel(\"mse\")\n",
                "    ax.set_title(\"mse vs history_len across models\")\n",
                "    for k,v in other_vals.items():\n",
                "        ax.axhline(y=v, color='gray', linestyle='--', alpha=0.7, label=k)\n",
                "\n",
                "    _add_legends(ax, df, marker_map, cdict, size_scale)\n",
                "    plt.tight_layout()\n",
                "    plt.show()\n",
                "\n",
                "\n",
                "# ----------------------------- grid plot -----------------------------\n",
                "from typing import Dict\n",
                "import math\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "from typing import Dict\n",
                "import math\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "def _plot_grid(  # noqa: N802\n",
                "    df: pd.DataFrame,\n",
                "    marker_map: dict[str, str],\n",
                "    cdict: dict[str, str],\n",
                "    *,\n",
                "    size_scale: float,\n",
                "    sync_axes: bool = True,\n",
                "    other_vals: dict[str, float] = {},\n",
                ") -> None:\n",
                "    extend_pct = 0.1\n",
                "    if sync_axes:\n",
                "        x_min, x_max = df[\"history_length\"].dropna().agg([\"min\", \"max\"])\n",
                "        y_min, y_max = df[\"mse\"].dropna().agg([\"min\", \"max\"])\n",
                "        if other_vals:\n",
                "            y_min = min(y_min, min(other_vals.values()))\n",
                "            y_max = max(y_max, max(other_vals.values()))\n",
                "        x_range = x_max - x_min\n",
                "        x_min, x_max = x_min - (x_range * extend_pct), x_max + (x_range * extend_pct)\n",
                "        y_range = y_max - y_min\n",
                "        y_min, y_max = y_min - (y_range * extend_pct), y_max + (y_range * extend_pct)\n",
                "        sharex = sharey = True\n",
                "    else:\n",
                "        sharex = sharey = False\n",
                "\n",
                "    models = sorted(df[\"model_name\"].unique())\n",
                "    n_mod = len(models)\n",
                "    side = math.ceil(math.sqrt(n_mod))\n",
                "    bottom_row = (n_mod - 1) // side  # last visible row\n",
                "\n",
                "    fig, axes = plt.subplots(\n",
                "        side,\n",
                "        side,\n",
                "        figsize=(6 * side, 6 * side),\n",
                "        sharex=sharex,\n",
                "        sharey=sharey,\n",
                "    )\n",
                "\n",
                "    axes_flat = axes.ravel()\n",
                "    for ax in axes_flat[n_mod:]:\n",
                "        ax.set_visible(False)\n",
                "\n",
                "    for idx, (ax, model) in enumerate(zip(axes_flat, models)):\n",
                "        sdf = df[df[\"model_name\"] == model]\n",
                "        for choice, sub in sdf.groupby(\"history_choice\"):\n",
                "            ax.scatter(\n",
                "                sub[\"history_length\"],\n",
                "                sub[\"mse\"],\n",
                "                s=_effective_size(sub[\"expected_frames\"], size_scale).astype(float),\n",
                "                marker=marker_map[model],\n",
                "                color=cdict[choice],\n",
                "                alpha=0.8,\n",
                "                label=str(choice),  # labels grabbed later\n",
                "            )\n",
                "        \n",
                "        for k,v in other_vals.items():\n",
                "            ax.axhline(y=v, color='gray', linestyle='--', alpha=0.7, label=k)\n",
                "\n",
                "        ax.set_title(model)\n",
                "        if sync_axes:\n",
                "            ax.set_xlim(x_min, x_max)\n",
                "            ax.set_ylim(y_min, y_max)\n",
                "\n",
                "        row, col = divmod(idx, side)\n",
                "        if col == 0:\n",
                "            ax.set_ylabel(\"mse\")\n",
                "        \n",
                "        ax.set_xlabel(\"history_len (# frames)\")\n",
                "        ax.tick_params(axis=\"x\", labelbottom=True)   # show xtick labels here\n",
                "\n",
                "    # build combined legend once and shove it outside\n",
                "    handles, labels = axes_flat[0].get_legend_handles_labels()\n",
                "    fig.legend(handles, labels, loc=\"center left\", bbox_to_anchor=(1.02, 0.5))\n",
                "\n",
                "    plt.tight_layout()\n",
                "    plt.subplots_adjust(right=0.82)  # leave room for legend\n",
                "    plt.show()\n",
                "\n",
                "\n",
                "\n",
                "\n",
                "# ----------------------------- legend builder -----------------------------\n",
                "\n",
                "def _add_legends(ax, df: pd.DataFrame, marker_map: dict[str, str], cdict: dict[str, str], size_scale: float) -> None:\n",
                "    choices = sorted(df[\"history_choice\"].unique())\n",
                "\n",
                "    # 1) color legend (history_choice)\n",
                "    color_handles = [\n",
                "        Line2D([], [], marker=\"o\", linestyle=\"\", markerfacecolor=cdict[c], markeredgecolor=\"none\", markersize=10, label=c)\n",
                "        for c in choices\n",
                "    ]\n",
                "\n",
                "    # 2) size legend (expected_frames)\n",
                "    expected_frames = df[\"expected_frames\"]\n",
                "    size_vals = _legend_size_values(expected_frames.astype(float).values)\n",
                "    # size_handles = [\n",
                "    #     ax.scatter([], [], s=_effective_size(v, size_scale), color=\"gray\", alpha=0.6, label=f\"{v} frames\")\n",
                "    #     for v in size_vals\n",
                "    # ]\n",
                "    size_handles = [\n",
                "        Line2D([], [], marker=\"o\", linestyle=\"\", color=\"gray\", markersize=_effective_size(v, size_scale)/10, label=f\"{v} frames\")\n",
                "        for v in size_vals\n",
                "    ]\n",
                "\n",
                "    # 3) marker legend (model_name)\n",
                "    marker_handles = [\n",
                "        Line2D([], [], marker=marker_map[m], linestyle=\"\", color=\"gray\", markersize=10, label=m)\n",
                "        for m in sorted(marker_map)\n",
                "    ]\n",
                "\n",
                "    legend_kw = dict(loc=\"upper left\", bbox_to_anchor=(1.02, 1.1), frameon=False, alignment=\"left\")\n",
                "    first = ax.legend(handles=color_handles, title=\"History Choice\", **legend_kw)\n",
                "    ax.add_artist(first)\n",
                "\n",
                "    legend_kw[\"bbox_to_anchor\"] = (1.02, 0.45)\n",
                "    second = ax.legend(handles=size_handles, title=\"Expected # Frames\", **legend_kw)\n",
                "    ax.add_artist(second)\n",
                "\n",
                "\n",
                "    legend_kw[\"bbox_to_anchor\"] = (1.02, 0.0)\n",
                "    ax.legend(handles=marker_handles, title=\"Model Name\", **legend_kw)\n",
                "\n",
                "\n",
                "    \n",
                "    \n",
                "# ----------------------------- public API -----------------------------\n",
                "\n",
                "def plot_mse_by_model(\n",
                "    df: pd.DataFrame,\n",
                "    *,\n",
                "    grid: bool = False,\n",
                "    sync_axes: bool = True,\n",
                "    size_scale: float = 20.0,\n",
                "    other_vals: dict[str, float] = {},\n",
                ") -> None:\n",
                "    \"\"\"plot mse vs history_length.\n",
                "\n",
                "    parameters\n",
                "    ----------\n",
                "    df : pd.DataFrame\n",
                "        requires columns `{\"model_name\", \"history_length\", \"mse\", \"history_choice\", \"expected_frames\"}.\n",
                "    grid : bool, default `False\n",
                "        True → one subplot per model in a square grid.\n",
                "    size_scale : float, default `20.0\n",
                "        multiplier converting frames → marker points²; zero frames mapped to size of *1 frame*.\n",
                "    \"\"\"\n",
                "\n",
                "    required = {\"model_name\", \"history_length\", \"mse\", \"history_choice\", \"expected_frames\"}\n",
                "    if not required.issubset(df.columns):\n",
                "        raise ValueError(f\"df missing columns: {required.difference(df.columns)}\")\n",
                "\n",
                "    models = sorted(df[\"model_name\"].unique())\n",
                "    marker_map = _build_marker_map(models)\n",
                "\n",
                "    choices = sorted(df[\"history_choice\"].unique())\n",
                "    colors = plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]\n",
                "    cdict: dict[str, str] = {c: colors[i % len(colors)] for i, c in enumerate(choices)}\n",
                "\n",
                "    if grid:\n",
                "        _plot_grid(df, marker_map, cdict, sync_axes=sync_axes, size_scale=size_scale, other_vals=other_vals)\n",
                "    else:\n",
                "        _plot_single(df, marker_map, cdict, size_scale=size_scale, other_vals=other_vals)\n",
                "\n",
                "\n",
                "plot_mse_by_model(comparison_df.T, grid=False, other_vals={\"openvla\": BASELINE_METRICS['openvla']['mse']})"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "plot_mse_by_model(comparison_df.T, grid=True, other_vals={\"openvla\": BASELINE_METRICS['openvla']['mse']})"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "lapa",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.15"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
