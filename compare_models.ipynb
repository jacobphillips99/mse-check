{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Comparison for Robot Policy Performance\n",
    "\n",
    "This notebook compares the performance of different models tested using the `test_policy_client_mse.py` script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from typing import Dict, List, Any\n",
    "\n",
    "# Set plot style\n",
    "plt.style.use('ggplot')\n",
    "sns.set_context(\"talk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths to model action files\n",
    "model_paths = {\n",
    "    \"openvla\": \"results/actions_openvla_20250421_152240.pkl\",\n",
    "    \"o4-mini\": \"results/actions_o4-mini_20250421_152435.pkl\",\n",
    "    \"gpt-4o\": \"results/actions_gpt-4o_20250421_180623.pkl\",\n",
    "    # \"gpt-4o-tunnel\": \"results/actions_gpt-4o-tunnel_20250423_134722.pkl\",\n",
    "    # \"gpt-4o-tunnel-async\": \"results/actions_gpt-4o-tunnel_20250423_140019.pkl\",\n",
    "    # \"gpt-4o-mini\": \"results/actions_gpt-4o-mini_20250423_152620.pkl\",\n",
    "    # \"gpt-4o-mini-tunnel-async\": \"results/actions_gpt-4o-mini-async_20250423_140435.pkl\",\n",
    "    # \"gpt-4o-mini-async\": \"results/actions_gpt-4o-mini_20250423_141040.pkl\",\n",
    "    # \"gemini-2.0-flash-tunnel\": \"results/actions_gemini-2.0-flash-tunnel_20250423_142234.pkl\",\n",
    "    \"o3\": \"results/actions_o3_20250421_181454.pkl\",\n",
    "    \"gemini-2.5-pro\": \"results/actions_gemini-2-5-pro_20250421_172434.pkl\",\n",
    "    \"gemini-2.5-pro-history1\": \"results/actions_gemini-2-5-pro-history1_20250429_151242.pkl\",\n",
    "    # \"gemini-2.5-flash\": \"results/actions_gemini-2.5-flash_20250421_180103.pkl\",\n",
    "    # \"gemini-2.5-flash-history1\": \"results/actions_gemini-2-5-flash-history1_20250429_154648.pkl\",\n",
    "    \"gemini-2.5-pro-historytest\": \"results/actions_testhist_20250501_221849.pkl\",\n",
    "    \"gemini-2.5-pro-historytest2\": \"results/actions_testhist_20250501_222835.pkl\",\n",
    "    # \"gemini-2.5-flash-tunnel\": \"results/actions_gemini-2.5-flash-tunnel_20250423_133919.pkl\",\n",
    "    \"gemini-2-0-flash\": \"results/actions_gemini-2-0-flash_20250422_140401.pkl\",\n",
    "    # \"gemini-2.0-flash-tunnel-async\": \"results/actions_gemini-2.0-flash-tunnel-async_20250423_142053.pkl\",\n",
    "    \"claude-3-5-sonnet\": \"results/actions_claude-3-5-sonnet_20250422_135539.pkl\",\n",
    "    \"claude-3-7-sonnet\": \"results/actions_claude-3-7-sonnet_20250422_141227.pkl\",\n",
    "    # \"openvla-tunnel\": \"results/actions_openvla-tunnel_20250423_110538.pkl\",\n",
    "    # \"openvla-tunnel2\": \"results/actions_openvla-tunnel2_20250423_110927.pkl\"\n",
    "    # Add more models as needed\n",
    "    \"ecot\": \"results/actions_ecot-tunnel_20250424_173038.pkl\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load model data\n",
    "def load_model_data(model_paths):\n",
    "    model_data = {}\n",
    "    \n",
    "    for model_name, path in model_paths.items():\n",
    "        try:\n",
    "            # Try to load the actions file\n",
    "            with open(path, 'rb') as f:\n",
    "                model_data[model_name] = {\n",
    "                    'actions': pickle.load(f)\n",
    "                }\n",
    "                \n",
    "            # Try to load corresponding metrics file if it exists\n",
    "            metrics_path = path.replace('actions_', 'metrics_')\n",
    "            if os.path.exists(metrics_path):\n",
    "                with open(metrics_path, 'rb') as f:\n",
    "                    model_data[model_name]['metrics'] = pickle.load(f)\n",
    "            else:\n",
    "                print(f\"Warning: Metrics file not found for {model_name}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading data for {model_name}: {e}\")\n",
    "    \n",
    "    return model_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all model data\n",
    "model_data = load_model_data(model_paths)\n",
    "\n",
    "# Check which models were loaded successfully\n",
    "print(f\"Successfully loaded data for {len(model_data)} models:\")\n",
    "for model_name in model_data.keys():\n",
    "    print(f\"- {model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute metrics for models without pre-computed metrics\n",
    "def compute_metrics(pred_actions, gt_actions):\n",
    "    results = {}\n",
    "    \n",
    "    # MSE - Mean Squared Error\n",
    "    results[\"mse\"] = np.mean(np.square(pred_actions - gt_actions))\n",
    "    \n",
    "    # MAE - Mean Absolute Error\n",
    "    results[\"mae\"] = np.mean(np.abs(pred_actions - gt_actions))\n",
    "    \n",
    "    # Normalized MSE - divide by variance of ground truth\n",
    "    gt_var = np.var(gt_actions)\n",
    "    if gt_var > 0:\n",
    "        results[\"nmse\"] = results[\"mse\"] / gt_var\n",
    "    else:\n",
    "        results[\"nmse\"] = float('inf')\n",
    "    \n",
    "    # Action magnitude comparison\n",
    "    results[\"pred_mag\"] = np.mean(np.linalg.norm(pred_actions, axis=1))\n",
    "    results[\"gt_mag\"] = np.mean(np.linalg.norm(gt_actions, axis=1))\n",
    "    results[\"mag_ratio\"] = results[\"pred_mag\"] / results[\"gt_mag\"] if results[\"gt_mag\"] > 0 else float('inf')\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract and organize metrics across models for comparison\n",
    "def extract_comparison_metrics(model_data):\n",
    "    comparison = {}\n",
    "    \n",
    "    for model_name, data in model_data.items():\n",
    "        # If we have pre-computed metrics, use those\n",
    "        if 'metrics' in data and 'avg_metrics' in data['metrics']:\n",
    "            comparison[model_name] = data['metrics']['avg_metrics']\n",
    "        else:\n",
    "            # Otherwise, compute metrics from actions\n",
    "            print(f\"Computing metrics for {model_name}...\")\n",
    "            all_results = []\n",
    "            \n",
    "            for traj in data['actions']:\n",
    "                pred_actions = traj['pred_actions']\n",
    "                gt_actions = traj['gt_actions']\n",
    "                \n",
    "                results = compute_metrics(pred_actions, gt_actions)\n",
    "                \n",
    "                # Add timing information if available\n",
    "                if 'inference_times' in traj:\n",
    "                    results['mean_inference_time'] = np.mean(traj['inference_times'])\n",
    "                    \n",
    "                all_results.append(results)\n",
    "            \n",
    "            # Compute average metrics across all trajectories\n",
    "            avg_metrics = {k: np.mean([r[k] for r in all_results]) for k in all_results[0]}\n",
    "            comparison[model_name] = avg_metrics\n",
    "    \n",
    "    return comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract performance metrics for comparison\n",
    "comparison_metrics = extract_comparison_metrics(model_data)\n",
    "\n",
    "# Create a DataFrame for easier visualization\n",
    "metrics_of_interest = ['mse', 'mae', 'nmse', 'r2', 'cosine_sim', 'pearson_r', \n",
    "                       'pred_mag', 'gt_mag', 'mag_ratio', 'mean_inference_time']\n",
    "\n",
    "comparison_df = pd.DataFrame(index=metrics_of_interest)\n",
    "\n",
    "for model_name, metrics in comparison_metrics.items():\n",
    "    model_values = [metrics.get(metric, np.nan) for metric in metrics_of_interest]\n",
    "    comparison_df[model_name] = model_values\n",
    "\n",
    "# Display the comparison table\n",
    "comparison_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Metric Comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metric_comparison(comparison_df, metric_name, title=None, lower_is_better=True):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Get the data for this metric\n",
    "    metric_values = comparison_df.loc[metric_name]\n",
    "    \n",
    "    # Sort models by metric value\n",
    "    if lower_is_better:\n",
    "        sorted_models = metric_values.sort_values().index\n",
    "    else:\n",
    "        sorted_models = metric_values.sort_values(ascending=False).index\n",
    "    \n",
    "    # Create bar plot\n",
    "    bars = plt.bar(sorted_models, metric_values[sorted_models])\n",
    "    \n",
    "    # Add value labels on top of bars with smaller font size\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height + 0.001,\n",
    "                f'{height:.4f}', ha='center', va='bottom', rotation=0, fontsize=8)\n",
    "    \n",
    "    # Add title and labels\n",
    "    plt.title(title or f'{metric_name.upper()} Comparison Across Models')\n",
    "    plt.ylabel(metric_name)\n",
    "    plt.xlabel('Model')\n",
    "    \n",
    "    # Set rotated tick labels with proper alignment\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot MSE comparison (lower is better)\n",
    "plot_metric_comparison(comparison_df, 'mse', 'Mean Squared Error (lower is better)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Cosine Similarity comparison (higher is better)\n",
    "plot_metric_comparison(comparison_df, 'cosine_sim', 'Cosine Similarity (higher is better)', lower_is_better=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Inference Time comparison (lower is better)\n",
    "if 'mean_inference_time' in comparison_df.index:\n",
    "    # Convert to milliseconds for better readability\n",
    "    inference_times_ms = comparison_df.loc['mean_inference_time'] * 1000\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Sort models by inference time\n",
    "    sorted_models = inference_times_ms.sort_values().index\n",
    "    \n",
    "    # Create bar plot\n",
    "    bars = plt.bar(sorted_models, inference_times_ms[sorted_models])\n",
    "    \n",
    "    # Add value labels on top of bars with smaller font size\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height + 0.5,\n",
    "                f'{height:.1f} ms', ha='center', va='bottom', rotation=0, fontsize=8)\n",
    "    \n",
    "    plt.title('Mean Inference Time (lower is better)')\n",
    "    plt.ylabel('Time (ms)')\n",
    "    plt.xlabel('Model')\n",
    "    \n",
    "    # Set rotated tick labels with proper alignment\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    \n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Action Magnitude Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Improved action magnitude comparison plot with simple color scheme\n",
    "# Extract data\n",
    "models = list(comparison_metrics.keys())\n",
    "pred_mags = [comparison_metrics[model]['pred_mag'] for model in models]\n",
    "gt_mag = comparison_metrics[models[0]]['gt_mag']  # Ground truth should be the same for all models\n",
    "\n",
    "# Set up a simple color palette\n",
    "colors = plt.cm.tab10(np.arange(len(models)))\n",
    "\n",
    "# Create plot\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "bars = ax.bar(np.arange(len(models)), pred_mags, color=colors, alpha=0.8)\n",
    "\n",
    "# Add horizontal line for ground truth\n",
    "ax.axhline(y=gt_mag, color='red', linestyle='--', alpha=0.7, label=f'Ground Truth: {gt_mag:.4f}')\n",
    "\n",
    "# Add value labels on top of bars\n",
    "for i, bar in enumerate(bars):\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., \n",
    "            height + 0.01,\n",
    "            f'{height:.4f}', \n",
    "            ha='center', \n",
    "            va='bottom',\n",
    "            fontsize=10)\n",
    "\n",
    "# Set labels and title\n",
    "ax.set_xlabel('Model')\n",
    "ax.set_ylabel('Action Magnitude')\n",
    "ax.set_title('Action Magnitude Comparison')\n",
    "ax.set_xticks(np.arange(len(models)))\n",
    "ax.set_xticklabels(models, rotation=45, ha='right')\n",
    "ax.legend()\n",
    "\n",
    "# Add grid for better readability\n",
    "ax.grid(axis='y', linestyle='--', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trajectory-level Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract trajectory-level metrics for all models\n",
    "def get_trajectory_metrics(model_data, metric_name='mse'):\n",
    "    traj_metrics = {}\n",
    "    \n",
    "    for model_name, data in model_data.items():\n",
    "        # If we have pre-computed metrics, use those\n",
    "        if 'metrics' in data and 'results' in data['metrics']:\n",
    "            traj_metrics[model_name] = [r.get(metric_name, np.nan) for r in data['metrics']['results']]\n",
    "        else:\n",
    "            # Otherwise, compute metrics from actions\n",
    "            print(f\"Computing trajectory metrics for {model_name}...\")\n",
    "            model_traj_metrics = []\n",
    "            \n",
    "            for traj in data['actions']:\n",
    "                pred_actions = traj['pred_actions']\n",
    "                gt_actions = traj['gt_actions']\n",
    "                \n",
    "                if metric_name == 'mse':\n",
    "                    value = np.mean(np.square(pred_actions - gt_actions))\n",
    "                elif metric_name == 'mae':\n",
    "                    value = np.mean(np.abs(pred_actions - gt_actions))\n",
    "                else:\n",
    "                    # Compute the requested metric\n",
    "                    results = compute_metrics(pred_actions, gt_actions)\n",
    "                    value = results.get(metric_name, np.nan)\n",
    "                    \n",
    "                model_traj_metrics.append(value)\n",
    "            \n",
    "            traj_metrics[model_name] = model_traj_metrics\n",
    "    \n",
    "    return traj_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get MSE for each trajectory across models\n",
    "traj_mse = get_trajectory_metrics(model_data, 'mse')\n",
    "\n",
    "# Calculate average and median MSE for each trajectory (across all models)\n",
    "traj_indices = range(len(next(iter(traj_mse.values()))))\n",
    "avg_per_traj = [np.mean([traj_mse[model][i] for model in traj_mse.keys()]) for i in traj_indices]\n",
    "median_per_traj = [np.median([traj_mse[model][i] for model in traj_mse.keys()]) for i in traj_indices]\n",
    "\n",
    "# Plot trajectory-level MSE comparison\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "# Plot individual model lines\n",
    "for model_name, mse_values in traj_mse.items():\n",
    "    plt.plot(traj_indices, mse_values, marker='o', alpha=0.7, linewidth=1, label=model_name)\n",
    "\n",
    "# Plot average and median with thicker lines for emphasis\n",
    "plt.plot(traj_indices, avg_per_traj, 'k-', linewidth=3, label='Average MSE')\n",
    "plt.plot(traj_indices, median_per_traj, 'r--', linewidth=3, label='Median MSE')\n",
    "\n",
    "plt.title('MSE per Trajectory Across Models')\n",
    "plt.xlabel('Trajectory Index')\n",
    "plt.ylabel('MSE')\n",
    "plt.legend(loc='upper left', bbox_to_anchor=(1, 1))\n",
    "plt.grid(True)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create boxplot for comparison of MSE distribution across models\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Prepare data for boxplot\n",
    "box_data = [values for model, values in traj_mse.items()]\n",
    "box_labels = list(traj_mse.keys())\n",
    "\n",
    "plt.boxplot(box_data, labels=box_labels, patch_artist=True)\n",
    "plt.title('Distribution of MSE Across Trajectories')\n",
    "plt.ylabel('MSE')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimension-wise Action Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze MSE by action dimension\n",
    "def compute_dimension_mse(model_data):\n",
    "    dim_mse = {}\n",
    "    \n",
    "    # First, get action dimensions from the first model\n",
    "    first_model = next(iter(model_data.values()))\n",
    "    first_traj = first_model['actions'][0]\n",
    "    action_dims = first_traj['pred_actions'].shape[1]\n",
    "    \n",
    "    for model_name, data in model_data.items():\n",
    "        dim_mse[model_name] = []\n",
    "        \n",
    "        for dim in range(action_dims):\n",
    "            dim_errors = []\n",
    "            \n",
    "            for traj in data['actions']:\n",
    "                pred_dim = traj['pred_actions'][:, dim]\n",
    "                gt_dim = traj['gt_actions'][:, dim]\n",
    "                dim_errors.append(np.mean(np.square(pred_dim - gt_dim)))\n",
    "            \n",
    "            dim_mse[model_name].append(np.mean(dim_errors))\n",
    "    \n",
    "    return dim_mse, action_dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot dimension-wise MSE on separate subplots with improved styling\n",
    "def plot_dimension_mse_subplots(dim_mse, action_dims):\n",
    "    # Determine number of rows and columns for subplots\n",
    "    ncols = 2\n",
    "    nrows = (action_dims + ncols - 1) // ncols  # Ceiling division\n",
    "    \n",
    "    # Create figure with subplots\n",
    "    fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(16, 4*nrows))\n",
    "    axes = axes.flatten() if nrows > 1 or ncols > 1 else [axes]\n",
    "    \n",
    "    # Define a colormap for consistent colors across models\n",
    "    colors = plt.cm.tab10.colors  # Get colors from tab10 colormap\n",
    "    model_names = list(dim_mse.keys())\n",
    "    model_colors = {model: colors[i % len(colors)] for i, model in enumerate(model_names)}\n",
    "    \n",
    "    # Plot each dimension in its own subplot\n",
    "    for dim in range(action_dims):\n",
    "        ax = axes[dim]\n",
    "        \n",
    "        # Extract MSE values for this dimension across all models\n",
    "        mse_values = [dim_mse[model][dim] for model in model_names]\n",
    "        \n",
    "        # Create bar plot for this dimension with custom colors\n",
    "        bars = ax.bar(model_names, mse_values, color=[model_colors[model] for model in model_names])\n",
    "        \n",
    "        # Add value labels on top of bars with better formatting\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            # Format the text based on the magnitude\n",
    "            if height < 0.001:\n",
    "                text = f'{height:.2e}'  # Scientific notation for very small values\n",
    "            elif height < 0.01:\n",
    "                text = f'{height:.5f}'\n",
    "            else:\n",
    "                text = f'{height:.3f}'\n",
    "                \n",
    "            ax.text(bar.get_x() + bar.get_width()/2., height * 1.02,\n",
    "                   text, ha='center', va='bottom', fontsize=10, \n",
    "                   rotation=0, fontweight='bold', color='black')\n",
    "        \n",
    "        # Set title and labels with better styling\n",
    "        ax.set_title(f'Dimension {dim}', fontsize=14, pad=10)\n",
    "        ax.set_ylabel('MSE', fontsize=12)\n",
    "        \n",
    "        # Improve x-tick labels\n",
    "        ax.set_xticklabels(model_names, rotation=30, ha='right', fontsize=10)\n",
    "        \n",
    "        # Improve grid and background\n",
    "        ax.grid(axis='y', linestyle='--', alpha=0.3)\n",
    "        ax.set_axisbelow(True)  # Put grid lines behind bars\n",
    "        \n",
    "        # Add light background color\n",
    "        ax.set_facecolor('#f8f8f8')\n",
    "    \n",
    "    # Hide any unused subplots\n",
    "    for i in range(action_dims, len(axes)):\n",
    "        axes[i].set_visible(False)\n",
    "    \n",
    "    # Add a single legend for all subplots\n",
    "    handles = [plt.Rectangle((0,0), 1, 1, color=model_colors[model]) for model in model_names]\n",
    "    fig.legend(handles, model_names, loc='upper center', bbox_to_anchor=(0.5, 0), \n",
    "              ncol=min(5, len(model_names)), fontsize=12, frameon=True, facecolor='white')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.suptitle('MSE by Action Dimension (Each plot has its own scale)', \n",
    "                y=1.02, fontsize=16, fontweight='bold')\n",
    "    plt.subplots_adjust(top=0.92, bottom=0.15)  # Make room for the legend at the bottom\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Compute dimension-wise MSE\n",
    "dim_mse, action_dims = compute_dimension_mse(model_data)\n",
    "# Use this function to plot the dimension-wise MSE\n",
    "plot_dimension_mse_subplots(dim_mse, action_dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pred_actions = {k: [tj['pred_actions'] for tj in v['actions']] for k,v in model_data.items()}\n",
    "gt_actions = {k: [tj['gt_actions'] for tj in v['actions']] for k,v in model_data.items()}\n",
    "\n",
    "pred_actions = {k: np.concatenate(v, axis=0 ) for k,v in pred_actions.items()}\n",
    "gt_actions = {k: np.concatenate(v, axis=0) for k,v in gt_actions.items()}\n",
    "\n",
    "for k in pred_actions.keys():\n",
    "    print(f\"pred: {k}, {pred_actions[k].shape}\")\n",
    "    print(f\"gt: {k}, {gt_actions[k].shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_action_dimensions(pred_actions, gt_actions, model_names=None):\n",
    "    \"\"\"\n",
    "    Plot each dimension of the action data over time steps.\n",
    "    \n",
    "    Args:\n",
    "        pred_actions: Dictionary mapping model names to predicted action arrays\n",
    "        gt_actions: Dictionary mapping model names to ground truth action arrays\n",
    "        model_names: Optional list of model names to plot (if None, plots all)\n",
    "    \"\"\"\n",
    "    if model_names is None:\n",
    "        model_names = list(pred_actions.keys())\n",
    "    \n",
    "    # Get the action dimensions from the first model\n",
    "    first_model = next(iter(pred_actions.values()))\n",
    "    action_dims = first_model.shape[1]\n",
    "    \n",
    "    # Create a figure for each action dimension\n",
    "    for dim in range(action_dims):\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        \n",
    "        # Plot predicted actions for each model\n",
    "        for model in model_names:\n",
    "            # Get time steps for x-axis\n",
    "            time_steps = np.arange(len(pred_actions[model]))\n",
    "            \n",
    "            # Plot predicted actions\n",
    "            plt.plot(time_steps, pred_actions[model][:, dim], \n",
    "                     label=f\"{model} (pred)\", alpha=0.7)\n",
    "            \n",
    "            # Plot ground truth actions (only once since it's the same for all models)\n",
    "            if model == model_names[0]:\n",
    "                plt.plot(time_steps, gt_actions[model][:, dim], \n",
    "                         label=\"Ground Truth\", color='black', linestyle='--', linewidth=2)\n",
    "        \n",
    "        plt.title(f\"Action Dimension {dim}\")\n",
    "        plt.xlabel(\"Time Step\")\n",
    "        plt.ylabel(\"Action Value\")\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "plot_action_dimensions(pred_actions, gt_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics_vs_latency(comparison_df, metrics=None, figsize=(14, 10), save_path=None):\n",
    "    \"\"\"\n",
    "    Create scatterplots of metrics vs latency\n",
    "    \n",
    "    Args:\n",
    "        comparison_df: DataFrame with models as columns and metrics as rows\n",
    "        metrics: List of metrics to plot against latency (default: ['mse'])\n",
    "        figsize: Size of the figure (default: (14, 10))\n",
    "        save_path: Path to save the plot (default: None)\n",
    "    \"\"\"\n",
    "    if metrics is None:\n",
    "        metrics = ['mse']\n",
    "    \n",
    "    # Transpose to get models as rows\n",
    "    df_transposed = comparison_df.T\n",
    "    \n",
    "    # Convert latency to milliseconds\n",
    "    df_transposed['latency_ms'] = df_transposed['mean_inference_time'] * 1000\n",
    "    \n",
    "    # Set up the plot grid\n",
    "    n_metrics = len(metrics)\n",
    "    n_cols = min(2, n_metrics)\n",
    "    n_rows = (n_metrics + n_cols - 1) // n_cols  # Ceiling division\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=figsize, squeeze=False)\n",
    "    sns.set(style=\"whitegrid\")\n",
    "    \n",
    "    # Flatten the axes array for easier indexing\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    # Get unique colors for each model\n",
    "    colors = sns.color_palette(\"husl\", len(df_transposed))\n",
    "    \n",
    "    # Plot each metric\n",
    "    for i, metric in enumerate(metrics):\n",
    "        if i < len(axes):\n",
    "            ax = axes[i]\n",
    "            \n",
    "            # Check if the metric exists in the DataFrame\n",
    "            if metric not in df_transposed.columns:\n",
    "                ax.text(0.5, 0.5, f\"Metric '{metric}' not found\", \n",
    "                        ha='center', va='center', transform=ax.transAxes)\n",
    "                continue\n",
    "            \n",
    "            # Plot the points\n",
    "            for j, (model, row) in enumerate(df_transposed.iterrows()):\n",
    "                ax.scatter(\n",
    "                    row['latency_ms'], \n",
    "                    row[metric], \n",
    "                    s=100,  # marker size\n",
    "                    color=colors[j],\n",
    "                    label=model,\n",
    "                    alpha=0.7\n",
    "                )\n",
    "                \n",
    "                # Add model name as text label\n",
    "                ax.text(\n",
    "                    row['latency_ms'] * 1.03,  # slightly offset\n",
    "                    row[metric] * 1.03,  # slightly offset\n",
    "                    model,\n",
    "                    fontsize=9\n",
    "                )\n",
    "            \n",
    "            # Add title and labels\n",
    "            ax.set_title(f'{metric.upper()} vs Latency', fontsize=14)\n",
    "            ax.set_xlabel('Latency (ms)', fontsize=12)\n",
    "            ax.set_ylabel(metric.upper(), fontsize=12)\n",
    "            \n",
    "            # Rotate x-axis tick labels to prevent overlap\n",
    "            plt.setp(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "            \n",
    "            # Add grid\n",
    "            ax.grid(True, linestyle='--', alpha=0.7)\n",
    "            \n",
    "            # Annotate the ideal region (low metric, low latency)\n",
    "            min_latency = df_transposed['latency_ms'].min()\n",
    "            min_metric = df_transposed[metric].min()\n",
    "            \n",
    "            ax.annotate('Better', \n",
    "                       xy=(min_latency, min_metric), \n",
    "                       xytext=(min_latency + 5, min_metric * 0.7),\n",
    "                       arrowprops=dict(facecolor='green', shrink=0.05, width=2),\n",
    "                       fontsize=12, color='green')\n",
    "    \n",
    "    # Hide unused subplots\n",
    "    for j in range(i+1, len(axes)):\n",
    "        axes[j].axis('off')\n",
    "    \n",
    "    # Adjust layout with extra bottom margin for rotated labels\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.97])\n",
    "    \n",
    "    # Save if requested\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"Plot saved as '{save_path}'\")\n",
    "    \n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics_vs_latency(comparison_df, metrics=['mse', 'mae', 'r2', 'cosine_sim'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lapa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
